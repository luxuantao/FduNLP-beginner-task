{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    data_path = \"json/\"\n",
    "    category = \"poet.tang\"\n",
    "    author = None\n",
    "    constrain = None\n",
    "    poetry_max_len = 125\n",
    "    sample_max_len = poetry_max_len-1\n",
    "    processed_data_path = \"data/tang.npz\"\n",
    "    word_dict_path = 'wordDic'\n",
    "    model_path = 'model/tang_0.pth'\n",
    "    model_prefix = 'model/tang'\n",
    "    batch_size = 8\n",
    "    epoch_num = 1\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 256\n",
    "    layer_num = 2  # rnn的层数\n",
    "    lr = 0.01\n",
    "    weight_decay = 1e-4\n",
    "    plot_every = 2\n",
    "    env = 'poetry'\n",
    "    use_gpu = True\n",
    "    max_gen_len = 200  # 生成诗歌最长长度\n",
    "    sentence_max_len = 4 # 生成诗歌的最长句子\n",
    "    prefix_words = '细雨鱼儿出,微风燕子斜。'  # 不是诗歌的组成部分，用来控制生成诗歌的意境\n",
    "    start_words = '闲云潭影日悠悠'  # 诗歌开始\n",
    "    acrostic = False  # 是否是藏头诗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_data(data_path, category, author, constrain):\n",
    "    \"\"\"\n",
    "    获取原数据并预处理\n",
    "    :param data_path: 数据存放的路径\n",
    "    :param category: 数据的类型\n",
    "    :param author: 作者名称\n",
    "    :param constrain: 长度限制\n",
    "    :return: list\n",
    "    ['床前明月光，疑是地上霜，举头望明月，低头思故乡。',\n",
    "     '一去二三里，烟村四五家，亭台六七座，八九十支花。',\n",
    "    .........\n",
    "    ]\n",
    "    \"\"\"\n",
    "    def sentence_parse(para):\n",
    "        \"\"\"对文本进行处理，取出脏数据\"\"\"\n",
    "        # 去掉括号中的部分\n",
    "        # para = \"-181-村橋路不端，數里就迴湍。積壤連涇脉，高林上笋竿。早嘗甘蔗淡，生摘琵琶酸。（「琵琶」，嚴壽澄校《張祜詩集》云：疑「枇杷」之誤。）好是去塵俗，煙花長一欄。\"\n",
    "        result, number = re.subn(\"（.*）\", \"\", para)\n",
    "        result, number = re.subn(\"{.*}\", \"\", result)\n",
    "        result, number = re.subn(\"《.*》\", \"\", result)\n",
    "        result, number = re.subn(\"《.*》\", \"\", result)\n",
    "        result, number = re.subn(\"[\\]\\[]\", \"\", result)\n",
    "        # 去掉数字\n",
    "        r = \"\"\n",
    "        for s in result:\n",
    "            if s not in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-']:\n",
    "                r += s;\n",
    "        # 处理两个句号为1个句号\n",
    "        r, number = re.subn(\"。。\", \"。\", r)\n",
    "        # 返回预处理好的文本\n",
    "        return r\n",
    "\n",
    "    def handle_json(file):\n",
    "        \"\"\"读入json文件，返回诗句list，每一个元素为一首诗歌(str类型表示)\"\"\"\n",
    "        rst = []\n",
    "        data = json.load(open(file, 'r', encoding='utf-8'))\n",
    "        for poetry in data:\n",
    "            pdata = \"\"\n",
    "            if author is not None and poetry.get(\"author\") != author:\n",
    "                continue\n",
    "            p = poetry.get(\"paragraphs\")\n",
    "            flag = False\n",
    "            for s in p:\n",
    "                sp = re.split(\"[，！。]\", s)\n",
    "                for tr in sp:\n",
    "                    if constrain is not None and len(tr) != constrain and len(tr) != 0:\n",
    "                        flag = True\n",
    "                        break\n",
    "                    if flag:\n",
    "                        break\n",
    "            if flag:\n",
    "                continue\n",
    "            for sentence in poetry.get(\"paragraphs\"):\n",
    "                pdata += sentence\n",
    "            pdata = sentence_parse(pdata)\n",
    "            if pdata != \"\" and len(pdata) > 1:\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "\n",
    "    data = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.startswith(category):\n",
    "            data += handle_json(data_path + filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences,\n",
    "                  maxlen=None,\n",
    "                  dtype='int32',\n",
    "                  padding='pre',\n",
    "                  truncating='pre',\n",
    "                  value=0.):\n",
    "    \"\"\"\n",
    "    code from keras\n",
    "    Pads each sequence to the same length (length of the longest sequence).\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    Arguments:\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    Returns:\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    Raises:\n",
    "        ValueError: in case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:  \n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):  \n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]  \n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError(\n",
    "                'Shape of sample %s of sequence at position %s is different from '\n",
    "                'expected shape %s'\n",
    "                % (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    # 1.获取数据\n",
    "    data = parse_raw_data(config.data_path, config.category, config.author, config.constrain)\n",
    "    # print(len(data), data[0])\n",
    "\n",
    "    # 2.构建词典\n",
    "    chars = {c for line in data for c in line}\n",
    "    char_to_ix = {char: ix for ix, char in enumerate(chars)}\n",
    "    char_to_ix['<EOP>'] = len(char_to_ix)\n",
    "    char_to_ix['<START>'] = len(char_to_ix)\n",
    "    char_to_ix['</s>'] = len(char_to_ix)\n",
    "    ix_to_chars = {ix: char for char, ix in char_to_ix.items()}\n",
    "\n",
    "    # 3.处理样本\n",
    "    # 3.1 每首诗加上首位符号\n",
    "    for i in range(len(data)):\n",
    "        data[i] = ['<START>'] + list(data[i]) + ['<EOP>']\n",
    "    # 3.2 文字转id\n",
    "    data_id = [[char_to_ix[w] for w in line] for line in data]\n",
    "    # 3.3 补全既定长度\n",
    "    pad_data = pad_sequences(data_id,\n",
    "                             maxlen=config.poetry_max_len,\n",
    "                             padding='pre',\n",
    "                             truncating='post',\n",
    "                             value=len(char_to_ix) - 1)\n",
    "    # 3.4 保存于返回\n",
    "    np.savez_compressed(config.processed_data_path,\n",
    "                        data=pad_data,\n",
    "                        word2ix=char_to_ix,\n",
    "                        ix2word=ix_to_chars)\n",
    "\n",
    "    return pad_data, char_to_ix, ix_to_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "pad_data, char_to_ix, ix_to_chars = get_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218\n",
      " 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218\n",
      " 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218\n",
      " 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218\n",
      " 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218 9218\n",
      " 9218 9218 9218 9218 9218 9217 3592 3065 3575 9011  809 5603 7982 3516\n",
      " 6774 5371 2721 1134  581 6614 4838 6191 5636 5603  201 4642 6459 5911\n",
      " 4395 1134  888  605 5699  536 4386 5603 1237 2432 9146 8678 7183 1134\n",
      " 6772 5404  486 1748 5412 5603 5940  816 6454  581 7816 1134 9216]\n",
      "蛻 0\n",
      "0 蛻\n"
     ]
    }
   ],
   "source": [
    "print(pad_data[0])\n",
    "\n",
    "for k, v in char_to_ix.items():\n",
    "    print(k, v)\n",
    "    break\n",
    "\n",
    "for k, v in ix_to_chars.items():\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, device, layer_num):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=layer_num)\n",
    "        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)\n",
    "        # 创建一个dropout层，训练时作用在线性层防止过拟合\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        seq_len, batch_size = inputs.size()\n",
    "        # 将one-hot形式的input在嵌入矩阵中转换成嵌入向量，torch.Size([length, batch_size, embedding_size])\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # output:torch.Size([length, batch_size, hidden_idm]), 每一个step的输出\n",
    "        # hidden: tuple(torch.Size([layer_num, 32, 256]) torch.Size([1, 32, 256])) # 最后一层输出的ct 和 ht, 在这里是没有用的\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        # 经过线性层，relu激活层 先转换成（max_len*batch_size, 256)维度，再过线性层（length, vocab_size)\n",
    "        output = F.relu(self.linear1(output.view(seq_len*batch_size, -1)))\n",
    "        # 输出最终结果，与hidden结果\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, layer_num, batch_size):\n",
    "        return (torch.zeros(layer_num, batch_size, self.hidden_dim, requires_grad=True).to(self.device),\n",
    "                torch.zeros(layer_num, batch_size, self.hidden_dim, requires_grad=True).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(object):\n",
    "    def __init__(self):\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        self.config = Config()\n",
    "        self.device = torch.device('cuda') if self.config.use_gpu else torch.device('cpu')\n",
    "\n",
    "    def train(self, data_loader, model, optimizer, criterion, char_to_ix, ix_to_chars):\n",
    "        for epoch in range(self.config.epoch_num):\n",
    "            for step, x in enumerate(data_loader):\n",
    "                # 1.处理数据\n",
    "                # x: (batch_size,max_len) ==> (max_len, batch_size)\n",
    "                x = x.long().transpose(1, 0).contiguous()\n",
    "                x = x.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                # input,target:  (max_len-1, batch_size)\n",
    "                input_, target = x[:-1, :], x[1:, :]\n",
    "                target = target.view(-1)\n",
    "                # 初始化hidden为(c0, h0): ((layer_num， batch_size, hidden_dim)，(layer_num， batch_size, hidden_dim)）\n",
    "                hidden = model.init_hidden(self.config.layer_num, x.size()[1])\n",
    "                # 2.前向计算\n",
    "                output, _ = model(input_, hidden)\n",
    "                # output:(max_len*batch_size,vocab_size), target:(max_len*batch_size)\n",
    "                loss = criterion(output, target) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if step % 200 == 0:\n",
    "                    print('epoch: %d,loss: %f' % (epoch, loss.item()))\n",
    "            if epoch % 1 == 0:\n",
    "                # 保存模型\n",
    "                torch.save(model.state_dict(), '%s_%s.pth' % (self.config.model_prefix, epoch))\n",
    "                # 分别以这几个字作为诗歌的第一个字，生成一首藏头诗\n",
    "                word = '春江花月夜凉如水'\n",
    "                gen_poetry = ''.join(self.generate_head_test(model, word, char_to_ix, ix_to_chars))\n",
    "                print(gen_poetry)\n",
    "\n",
    "    def run(self):\n",
    "        # 1 获取数据\n",
    "        data, char_to_ix, ix_to_chars = get_data(self.config)\n",
    "        vocab_size = len(char_to_ix)\n",
    "        print('样本数：%d' % len(data))\n",
    "        print('词典大小： %d' % vocab_size)\n",
    "        # 2 设置dataloader\n",
    "        data = torch.from_numpy(data)\n",
    "        data_loader = Data.DataLoader(data,\n",
    "                                      batch_size=self.config.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=1)\n",
    "        # 3 创建模型\n",
    "        model = PoetryModel(vocab_size=vocab_size,\n",
    "                            embedding_dim=self.config.embedding_dim,\n",
    "                            hidden_dim=self.config.hidden_dim,\n",
    "                            device=self.device,\n",
    "                            layer_num=self.config.layer_num)\n",
    "        model.to(self.device)\n",
    "        # 4 创建优化器\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay)\n",
    "        # 5 创建损失函数,使用与logsoftmax的输出\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # 6.训练\n",
    "        self.train(data_loader, model, optimizer, criterion, char_to_ix, ix_to_chars)\n",
    "\n",
    "    def generate_head_test(self, model, head_sentence, word_to_ix, ix_to_word):\n",
    "        \"\"\"生成藏头诗\"\"\"\n",
    "        poetry = []\n",
    "        head_char_len = len(head_sentence)  # 要生成的句子的数量\n",
    "        sentence_len = 0  # 当前句子的数量\n",
    "        pre_char = '<START>'  # 前一个已经生成的字\n",
    "        # 准备第一步要输入的数据\n",
    "        input = (torch.Tensor([word_to_ix['<START>']]).view(1, 1).long()).to(self.device)\n",
    "        hidden = model.init_hidden(self.config.layer_num, 1)\n",
    "\n",
    "        for i in range(self.config.max_gen_len):\n",
    "            # 前向计算出概率最大的当前词\n",
    "            output, hidden = model(input, hidden)\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            char = ix_to_word[top_index]\n",
    "            # 句首的字用藏头字代替\n",
    "            if pre_char in ['。', '！', '<START>']:\n",
    "                if sentence_len == head_char_len:\n",
    "                    break\n",
    "                else:\n",
    "                    char = head_sentence[sentence_len]\n",
    "                    sentence_len += 1\n",
    "                    input = (input.data.new([word_to_ix[char]])).view(1,1)\n",
    "            else:\n",
    "                input = (input.data.new([top_index])).view(1,1)\n",
    "            poetry.append(char)\n",
    "            pre_char = char\n",
    "        return poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数：57363\n",
      "词典大小： 9219\n",
      "epoch: 0,loss: 9.140446\n",
      "epoch: 0,loss: 3.372333\n",
      "epoch: 0,loss: 2.357655\n",
      "epoch: 0,loss: 3.550220\n",
      "epoch: 0,loss: 3.850133\n",
      "epoch: 0,loss: 2.572953\n",
      "epoch: 0,loss: 2.696341\n",
      "epoch: 0,loss: 2.785728\n",
      "epoch: 0,loss: 2.756162\n",
      "epoch: 0,loss: 2.179537\n",
      "epoch: 0,loss: 3.390952\n",
      "epoch: 0,loss: 2.306022\n",
      "epoch: 0,loss: 1.800979\n",
      "epoch: 0,loss: 3.094452\n",
      "epoch: 0,loss: 3.434525\n",
      "epoch: 0,loss: 4.095837\n",
      "epoch: 0,loss: 4.393913\n",
      "epoch: 0,loss: 1.923944\n",
      "epoch: 0,loss: 2.625626\n",
      "epoch: 0,loss: 2.528784\n",
      "epoch: 0,loss: 2.187022\n",
      "epoch: 0,loss: 2.868070\n",
      "epoch: 0,loss: 1.867179\n",
      "epoch: 0,loss: 2.692230\n",
      "epoch: 0,loss: 2.141259\n",
      "epoch: 0,loss: 2.864788\n",
      "epoch: 0,loss: 1.918024\n",
      "epoch: 0,loss: 4.772787\n",
      "epoch: 0,loss: 2.187701\n",
      "epoch: 0,loss: 2.493193\n",
      "epoch: 0,loss: 3.668999\n",
      "epoch: 0,loss: 2.665974\n",
      "epoch: 0,loss: 2.667639\n",
      "epoch: 0,loss: 2.893961\n",
      "epoch: 0,loss: 2.527862\n",
      "epoch: 0,loss: 2.459997\n",
      "春日千，一，一日，人時。江日日，一時，一時。花日，一時，一時。月日，日時，一時。夜日，一時，一時。凉日，一日，人時。如日，一時，一時。水日，一日，人時。\n"
     ]
    }
   ],
   "source": [
    "model = TrainModel()\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(object):\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.device = torch.device('cuda') if self.config.use_gpu else torch.device('cpu')\n",
    "        self.processed_data_path = self.config.processed_data_path\n",
    "        self.model_path = self.config.model_path\n",
    "        self.max_len = self.config.max_gen_len\n",
    "        self.sentence_max_len = self.config.sentence_max_len\n",
    "        self.load_data()\n",
    "        self.load_model()\n",
    "\n",
    "    def load_data(self):\n",
    "        if os.path.exists(self.processed_data_path):\n",
    "            data = np.load(self.processed_data_path, allow_pickle=True)\n",
    "            self.data, self.word_to_ix, self.ix_to_word = data['data'], data['word2ix'].item(), data['ix2word'].item()\n",
    "\n",
    "    def load_model(self):\n",
    "        model = PoetryModel(len(self.word_to_ix),\n",
    "                            self.config.embedding_dim,\n",
    "                            self.config.hidden_dim,\n",
    "                            self.device,\n",
    "                            self.config.layer_num)\n",
    "#         map_location = lambda s, l: s\n",
    "#         model.load_state_dict(torch.load(self.config.model_path, map_location=map_location))\n",
    "        model.load_state_dict(torch.load(self.config.model_path))\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "    def generate_random(self, start_words='<START>'):\n",
    "        \"\"\"自由生成一首诗歌\"\"\"\n",
    "        poetry = []\n",
    "        sentence_len = 0\n",
    "        input = (torch.Tensor([self.word_to_ix[start_words]]).view(1, 1).long()).to(self.device)\n",
    "        hidden = self.model.init_hidden(self.config.layer_num, 1)\n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            # 前向计算出概率最大的当前词\n",
    "            output, hidden = self.model(input, hidden)\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            char = self.ix_to_word[top_index]\n",
    "            # 遇到终结符则输出\n",
    "            if char == '<EOP>':\n",
    "                break\n",
    "            # 有8个句子则停止预测\n",
    "            if char in ['。', '！']:\n",
    "                sentence_len += 1\n",
    "                if sentence_len == 8:\n",
    "                    poetry.append(char)\n",
    "                    break\n",
    "            input = (input.data.new([top_index])).view(1, 1)\n",
    "            poetry.append(char)\n",
    "        return poetry\n",
    "\n",
    "    def generate_head(self, head_sentence):\n",
    "        \"\"\"生成藏头诗\"\"\"\n",
    "        poetry = []\n",
    "        head_char_len = len(head_sentence)  # 要生成的句子的数量\n",
    "        sentence_len = 0  # 当前句子的数量\n",
    "        pre_char = '<START>'  # 前一个已经生成的字\n",
    "        # 准备第一步要输入的数据\n",
    "        input = (torch.Tensor([self.word_to_ix['<START>']]).view(1, 1).long()).to(self.device)\n",
    "        hidden = self.model.init_hidden(self.config.layer_num, 1)\n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            # 前向计算出概率最大的当前词\n",
    "            output, hidden = self.model(input, hidden)\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            char = self.ix_to_word[top_index]\n",
    "            # 句首的字用藏头字代替\n",
    "            if pre_char in ['。', '！', '<START>']:\n",
    "                if sentence_len == head_char_len:\n",
    "                    break\n",
    "                else:\n",
    "                    char = head_sentence[sentence_len]\n",
    "                    sentence_len += 1\n",
    "                    input = (input.data.new([self.word_to_ix[char]])).view(1,1)\n",
    "            else:\n",
    "                input = (input.data.new([top_index])).view(1,1)\n",
    "            poetry.append(char)\n",
    "            pre_char = char\n",
    "        return poetry\n",
    "\n",
    "    def generate_poetry(self, mode=1, head_sentence=None):\n",
    "        \"\"\"\n",
    "        模式一：随机生成诗歌\n",
    "        模式二：生成藏头诗\n",
    "        \"\"\"\n",
    "        poetry = ''\n",
    "        if mode == 1 or (mode == 2 and head_sentence is None):\n",
    "            poetry = ''.join(self.generate_random())\n",
    "        if mode == 2 and head_sentence is not None:\n",
    "            head_sentence = head_sentence.replace(',', u'，').replace('.', u'。').replace('?', u'？')\n",
    "            poetry = ''.join(self.generate_head(head_sentence))\n",
    "        return poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一日千，一，一日一，人時，一日，一時。一日，一時，一時。一日，一日，人時。一日，一時，一時。一日，一日，人時。一日，一日，人時。一日，一時，一時。一日，一日，人時。\n"
     ]
    }
   ],
   "source": [
    "obj = Sample()\n",
    "poetry = obj.generate_poetry(mode=1)\n",
    "# poetry = obj.generate_poetry(mode=2, head_sentence=\"月的\")\n",
    "print(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
