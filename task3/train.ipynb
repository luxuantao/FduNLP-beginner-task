{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/47580077\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import time \n",
    "\n",
    "worddict_dir = \"data/worddict.txt\"\n",
    "data_train_id_dir = \"data/train_data_id.pkl\"\n",
    "data_dev_id_dir = \"data/dev_data_id.pkl\"\n",
    "embedding_matrix_dir = \"data/embedding_matrix.pkl\"\n",
    "model_train_dir = \"saved_model/train_model_\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SnliDataSet(Dataset):\n",
    "    def __init__(self, data, max_premise_len=None, max_hypothesis_len=None):\n",
    "        #序列长度\n",
    "        self.num_sequence = len(data[\"premise_id\"])\n",
    "        \n",
    "        #创建tensor矩阵的尺寸\n",
    "        self.premise_len = [len(seq) for seq in data[\"premise_id\"]]\n",
    "        self.max_premise_len = max_premise_len\n",
    "        if self.max_premise_len is None:\n",
    "            self.max_premise_len = max(self.premise_len)\n",
    "        \n",
    "        self.hypothesis_len = [len(seq) for seq in data[\"hypothesis_id\"]]\n",
    "        self.max_hypothesis_len = max_hypothesis_len\n",
    "        if max_hypothesis_len is None:\n",
    "            self.max_hypothesis_len = max(self.hypothesis_len)\n",
    "\n",
    "        #转成tensor，封装到data里\n",
    "        self.data = {\n",
    "            \"premise\": torch.zeros((self.num_sequence, self.max_premise_len), dtype=torch.long),\n",
    "            \"hypothesis\": torch.zeros((self.num_sequence, self.max_hypothesis_len), dtype=torch.long),\n",
    "            \"labels\": torch.tensor(data[\"labels_id\"])\n",
    "        }\n",
    "        \n",
    "        for i, premise in enumerate(data[\"premise_id\"]):\n",
    "            l = len(data[\"premise_id\"][i])\n",
    "            self.data[\"premise\"][i][:l] = torch.tensor(data[\"premise_id\"][i][:l])\n",
    "            l2 = len(data[\"hypothesis_id\"][i])\n",
    "            self.data[\"hypothesis\"][i][:l2] = torch.tensor(data[\"hypothesis_id\"][i][:l2])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_sequence\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return { \n",
    "            \"premise\": self.data[\"premise\"][index],\n",
    "            \"premise_len\": min(self.premise_len[index], self.max_premise_len),\n",
    "            \"hypothesis\": self.data[\"hypothesis\"][index],\n",
    "            \"hypothesis_len\": min(self.hypothesis_len[index], self.max_hypothesis_len),\n",
    "            \"labels\": self.data[\"labels\"][index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "batch_size = 512\n",
    "patience = 5\n",
    "hidden_size = 50\n",
    "dropout = 0.5\n",
    "num_classes = 3\n",
    "lr = 0.0004\n",
    "epochs = 1\n",
    "max_grad_norm = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "with open(data_train_id_dir, 'rb') as f:\n",
    "    train_data = SnliDataSet(pickle.load(f), max_premise_len=None, max_hypothesis_len=None)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "with open(data_dev_id_dir,'rb') as f:\n",
    "    dev_data = SnliDataSet(pickle.load(f), max_premise_len=None, max_hypothesis_len=None)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#加载embedding\n",
    "with open(embedding_matrix_dir,'rb') as f:\n",
    "    embeddings = torch.tensor(pickle.load(f), dtype=torch.float32).to(device) # 一定要是torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([33268, 50]), torch.float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.esim import ESIM\n",
    "\n",
    "model = ESIM(embeddings.shape[0], # 词汇表中单词个数\n",
    "             embeddings.shape[1], # 词向量维度\n",
    "             hidden_size,\n",
    "             embeddings=embeddings,\n",
    "             dropout=dropout,\n",
    "             num_classes=num_classes, # 输出为几类\n",
    "             device=device).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectNum(probs, targets):\n",
    "    _, out_classes = probs.max(dim=1) # 值，下标\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "def train(model, data_loader, optimizer, criterion, max_gradient_norm):\n",
    "    model.train()\n",
    "    device = model.device\n",
    "    time_epoch_start = time.time()\n",
    "    running_loss = 0 \n",
    "    correct_cnt = 0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    for index, batch in enumerate(data_loader):\n",
    "        time_batch_start = time.time()\n",
    "        premises = batch[\"premise\"].to(device)\n",
    "        premises_len = batch[\"premise_len\"].to(device)\n",
    "        hypothesis = batch[\"hypothesis\"].to(device)\n",
    "        hypothesis_len = batch[\"hypothesis_len\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits,probs = model(premises, premises_len, hypothesis, hypothesis_len)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        correct_cnt += getCorrectNum(probs, labels)\n",
    "        batch_cnt += 1\n",
    "        print(\"Training  ------>   Batch count: {:d}/{:d},  batch time: {:.4f}s,  batch average loss: {:.4f}\"\n",
    "              .format(batch_cnt, len(data_loader), time.time() - time_batch_start, running_loss / (index + 1)))\n",
    "        \n",
    "    epoch_time = time.time() - time_epoch_start\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_accuracy = correct_cnt / len(data_loader.dataset) \n",
    "    return epoch_time, epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    time_epoch_start = time.time()\n",
    "    running_loss = 0 \n",
    "    correct_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    for index, batch in enumerate(data_loader):\n",
    "        time_batch_start = time.time()\n",
    "        premises = batch[\"premise\"].to(device)\n",
    "        premises_len = batch[\"premise_len\"].to(device)\n",
    "        hypothesis = batch[\"hypothesis\"].to(device)\n",
    "        hypothesis_len = batch[\"hypothesis_len\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits, probs = model(premises, premises_len, hypothesis, hypothesis_len)\n",
    "        loss = criterion(logits, labels)\n",
    "        running_loss += loss.item()\n",
    "        correct_cnt += getCorrectNum(probs, labels)\n",
    "        \n",
    "        batch_cnt += 1\n",
    "        print(\"Testing  ------>   Batch count: {:d}/{:d},  batch time: {:.4f}s,  batch average loss: {:.4f}\"\n",
    "              .format(batch_cnt, len(data_loader), time.time() - time_batch_start, running_loss / (index + 1)))\n",
    "\n",
    "    epoch_time = time.time() - time_epoch_start\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_accuracy = correct_cnt / len(data_loader.dataset) \n",
    "    return epoch_time, epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- Training epoch 0 --------------------------------------------------\n",
      "Training  ------>   Batch count: 1/1073,  batch time: 0.3425s,  batch average loss: 1.1486\n",
      "Training  ------>   Batch count: 2/1073,  batch time: 0.3304s,  batch average loss: 1.1418\n",
      "Training  ------>   Batch count: 3/1073,  batch time: 0.3307s,  batch average loss: 1.1345\n",
      "Training  ------>   Batch count: 4/1073,  batch time: 0.3248s,  batch average loss: 1.1344\n",
      "Training  ------>   Batch count: 5/1073,  batch time: 0.3895s,  batch average loss: 1.1350\n",
      "Training  ------>   Batch count: 6/1073,  batch time: 0.3151s,  batch average loss: 1.1361\n",
      "Training  ------>   Batch count: 7/1073,  batch time: 0.3178s,  batch average loss: 1.1372\n",
      "Training  ------>   Batch count: 8/1073,  batch time: 0.3164s,  batch average loss: 1.1367\n",
      "Training  ------>   Batch count: 9/1073,  batch time: 0.3658s,  batch average loss: 1.1360\n",
      "Training  ------>   Batch count: 10/1073,  batch time: 0.3938s,  batch average loss: 1.1347\n",
      "Training  ------>   Batch count: 11/1073,  batch time: 0.3238s,  batch average loss: 1.1336\n",
      "Training  ------>   Batch count: 12/1073,  batch time: 0.2968s,  batch average loss: 1.1327\n",
      "Training  ------>   Batch count: 13/1073,  batch time: 0.3088s,  batch average loss: 1.1316\n",
      "Training  ------>   Batch count: 14/1073,  batch time: 0.3528s,  batch average loss: 1.1299\n",
      "Training  ------>   Batch count: 15/1073,  batch time: 0.2838s,  batch average loss: 1.1279\n",
      "Training  ------>   Batch count: 16/1073,  batch time: 0.3308s,  batch average loss: 1.1278\n",
      "Training  ------>   Batch count: 17/1073,  batch time: 0.3248s,  batch average loss: 1.1269\n",
      "Training  ------>   Batch count: 18/1073,  batch time: 0.3136s,  batch average loss: 1.1270\n",
      "Training  ------>   Batch count: 19/1073,  batch time: 0.3767s,  batch average loss: 1.1266\n",
      "Training  ------>   Batch count: 20/1073,  batch time: 0.3426s,  batch average loss: 1.1273\n",
      "Training  ------>   Batch count: 21/1073,  batch time: 0.3278s,  batch average loss: 1.1268\n",
      "Training  ------>   Batch count: 22/1073,  batch time: 0.3348s,  batch average loss: 1.1263\n",
      "Training  ------>   Batch count: 23/1073,  batch time: 0.3384s,  batch average loss: 1.1263\n",
      "Training  ------>   Batch count: 24/1073,  batch time: 0.2741s,  batch average loss: 1.1258\n",
      "Training  ------>   Batch count: 25/1073,  batch time: 0.3808s,  batch average loss: 1.1255\n",
      "Training  ------>   Batch count: 26/1073,  batch time: 0.3518s,  batch average loss: 1.1249\n",
      "Training  ------>   Batch count: 27/1073,  batch time: 0.3618s,  batch average loss: 1.1246\n",
      "Training  ------>   Batch count: 28/1073,  batch time: 0.3488s,  batch average loss: 1.1235\n",
      "Training  ------>   Batch count: 29/1073,  batch time: 0.3210s,  batch average loss: 1.1231\n",
      "Training  ------>   Batch count: 30/1073,  batch time: 0.3185s,  batch average loss: 1.1227\n",
      "Training  ------>   Batch count: 31/1073,  batch time: 0.3148s,  batch average loss: 1.1223\n",
      "Training  ------>   Batch count: 32/1073,  batch time: 0.4178s,  batch average loss: 1.1220\n",
      "Training  ------>   Batch count: 33/1073,  batch time: 0.3498s,  batch average loss: 1.1215\n",
      "Training  ------>   Batch count: 34/1073,  batch time: 0.3278s,  batch average loss: 1.1209\n",
      "Training  ------>   Batch count: 35/1073,  batch time: 0.3323s,  batch average loss: 1.1205\n",
      "Training  ------>   Batch count: 36/1073,  batch time: 0.3228s,  batch average loss: 1.1199\n",
      "Training  ------>   Batch count: 37/1073,  batch time: 0.3621s,  batch average loss: 1.1194\n",
      "Training  ------>   Batch count: 38/1073,  batch time: 0.2958s,  batch average loss: 1.1189\n",
      "Training  ------>   Batch count: 39/1073,  batch time: 0.3228s,  batch average loss: 1.1188\n",
      "Training  ------>   Batch count: 40/1073,  batch time: 0.3018s,  batch average loss: 1.1182\n",
      "Training  ------>   Batch count: 41/1073,  batch time: 0.4233s,  batch average loss: 1.1183\n",
      "Training  ------>   Batch count: 42/1073,  batch time: 0.3488s,  batch average loss: 1.1179\n",
      "Training  ------>   Batch count: 43/1073,  batch time: 0.3658s,  batch average loss: 1.1178\n",
      "Training  ------>   Batch count: 44/1073,  batch time: 0.3108s,  batch average loss: 1.1177\n",
      "Training  ------>   Batch count: 45/1073,  batch time: 0.3418s,  batch average loss: 1.1172\n",
      "Training  ------>   Batch count: 46/1073,  batch time: 0.3421s,  batch average loss: 1.1170\n",
      "Training  ------>   Batch count: 47/1073,  batch time: 0.3296s,  batch average loss: 1.1167\n",
      "Training  ------>   Batch count: 48/1073,  batch time: 0.3178s,  batch average loss: 1.1165\n",
      "Training  ------>   Batch count: 49/1073,  batch time: 0.3048s,  batch average loss: 1.1162\n",
      "Training  ------>   Batch count: 50/1073,  batch time: 0.3398s,  batch average loss: 1.1160\n",
      "Training  ------>   Batch count: 51/1073,  batch time: 0.3243s,  batch average loss: 1.1158\n",
      "Training  ------>   Batch count: 52/1073,  batch time: 0.3372s,  batch average loss: 1.1155\n",
      "Training  ------>   Batch count: 53/1073,  batch time: 0.2862s,  batch average loss: 1.1153\n",
      "Training  ------>   Batch count: 54/1073,  batch time: 0.3428s,  batch average loss: 1.1150\n",
      "Training  ------>   Batch count: 55/1073,  batch time: 0.3168s,  batch average loss: 1.1147\n",
      "Training  ------>   Batch count: 56/1073,  batch time: 0.4248s,  batch average loss: 1.1144\n",
      "Training  ------>   Batch count: 57/1073,  batch time: 0.3083s,  batch average loss: 1.1142\n",
      "Training  ------>   Batch count: 58/1073,  batch time: 0.3405s,  batch average loss: 1.1140\n",
      "Training  ------>   Batch count: 59/1073,  batch time: 0.4072s,  batch average loss: 1.1138\n",
      "Training  ------>   Batch count: 60/1073,  batch time: 0.3460s,  batch average loss: 1.1135\n",
      "Training  ------>   Batch count: 61/1073,  batch time: 0.3358s,  batch average loss: 1.1135\n",
      "Training  ------>   Batch count: 62/1073,  batch time: 0.3518s,  batch average loss: 1.1133\n",
      "Training  ------>   Batch count: 63/1073,  batch time: 0.3355s,  batch average loss: 1.1131\n",
      "Training  ------>   Batch count: 64/1073,  batch time: 0.3121s,  batch average loss: 1.1130\n",
      "Training  ------>   Batch count: 65/1073,  batch time: 0.2899s,  batch average loss: 1.1128\n",
      "Training  ------>   Batch count: 66/1073,  batch time: 0.3400s,  batch average loss: 1.1126\n",
      "Training  ------>   Batch count: 67/1073,  batch time: 0.3948s,  batch average loss: 1.1123\n",
      "Training  ------>   Batch count: 68/1073,  batch time: 0.3078s,  batch average loss: 1.1122\n",
      "Training  ------>   Batch count: 69/1073,  batch time: 0.3398s,  batch average loss: 1.1119\n",
      "Training  ------>   Batch count: 70/1073,  batch time: 0.3322s,  batch average loss: 1.1117\n",
      "Training  ------>   Batch count: 71/1073,  batch time: 0.3419s,  batch average loss: 1.1115\n",
      "Training  ------>   Batch count: 72/1073,  batch time: 0.3238s,  batch average loss: 1.1112\n",
      "Training  ------>   Batch count: 73/1073,  batch time: 0.4068s,  batch average loss: 1.1110\n",
      "Training  ------>   Batch count: 74/1073,  batch time: 0.4548s,  batch average loss: 1.1108\n",
      "Training  ------>   Batch count: 75/1073,  batch time: 0.3531s,  batch average loss: 1.1108\n",
      "Training  ------>   Batch count: 76/1073,  batch time: 0.3038s,  batch average loss: 1.1106\n",
      "Training  ------>   Batch count: 77/1073,  batch time: 0.3172s,  batch average loss: 1.1105\n",
      "Training  ------>   Batch count: 78/1073,  batch time: 0.3301s,  batch average loss: 1.1104\n",
      "Training  ------>   Batch count: 79/1073,  batch time: 0.3320s,  batch average loss: 1.1103\n",
      "Training  ------>   Batch count: 80/1073,  batch time: 0.3428s,  batch average loss: 1.1100\n",
      "Training  ------>   Batch count: 81/1073,  batch time: 0.3922s,  batch average loss: 1.1099\n",
      "Training  ------>   Batch count: 82/1073,  batch time: 0.3253s,  batch average loss: 1.1097\n",
      "Training  ------>   Batch count: 83/1073,  batch time: 0.3062s,  batch average loss: 1.1095\n",
      "Training  ------>   Batch count: 84/1073,  batch time: 0.3439s,  batch average loss: 1.1094\n",
      "Training  ------>   Batch count: 85/1073,  batch time: 0.3248s,  batch average loss: 1.1093\n",
      "Training  ------>   Batch count: 86/1073,  batch time: 0.2905s,  batch average loss: 1.1092\n",
      "Training  ------>   Batch count: 87/1073,  batch time: 0.3424s,  batch average loss: 1.1092\n",
      "Training  ------>   Batch count: 88/1073,  batch time: 0.3548s,  batch average loss: 1.1091\n",
      "Training  ------>   Batch count: 89/1073,  batch time: 0.3797s,  batch average loss: 1.1090\n",
      "Training  ------>   Batch count: 90/1073,  batch time: 0.3048s,  batch average loss: 1.1088\n",
      "Training  ------>   Batch count: 91/1073,  batch time: 0.3706s,  batch average loss: 1.1087\n",
      "Training  ------>   Batch count: 92/1073,  batch time: 0.3330s,  batch average loss: 1.1086\n",
      "Training  ------>   Batch count: 93/1073,  batch time: 0.3082s,  batch average loss: 1.1086\n",
      "Training  ------>   Batch count: 94/1073,  batch time: 0.2968s,  batch average loss: 1.1085\n",
      "Training  ------>   Batch count: 95/1073,  batch time: 0.3088s,  batch average loss: 1.1084\n",
      "Training  ------>   Batch count: 96/1073,  batch time: 0.3108s,  batch average loss: 1.1083\n",
      "Training  ------>   Batch count: 97/1073,  batch time: 0.3291s,  batch average loss: 1.1082\n",
      "Training  ------>   Batch count: 98/1073,  batch time: 0.3271s,  batch average loss: 1.1081\n",
      "Training  ------>   Batch count: 99/1073,  batch time: 0.3376s,  batch average loss: 1.1079\n",
      "Training  ------>   Batch count: 100/1073,  batch time: 0.4107s,  batch average loss: 1.1078\n",
      "Training  ------>   Batch count: 101/1073,  batch time: 0.2748s,  batch average loss: 1.1077\n",
      "Training  ------>   Batch count: 102/1073,  batch time: 0.3348s,  batch average loss: 1.1076\n",
      "Training  ------>   Batch count: 103/1073,  batch time: 0.3355s,  batch average loss: 1.1074\n",
      "Training  ------>   Batch count: 104/1073,  batch time: 0.4034s,  batch average loss: 1.1073\n",
      "Training  ------>   Batch count: 105/1073,  batch time: 0.3127s,  batch average loss: 1.1072\n",
      "Training  ------>   Batch count: 106/1073,  batch time: 0.2870s,  batch average loss: 1.1071\n",
      "Training  ------>   Batch count: 107/1073,  batch time: 0.3408s,  batch average loss: 1.1070\n",
      "Training  ------>   Batch count: 108/1073,  batch time: 0.2993s,  batch average loss: 1.1069\n",
      "Training  ------>   Batch count: 109/1073,  batch time: 0.3079s,  batch average loss: 1.1069\n",
      "Training  ------>   Batch count: 110/1073,  batch time: 0.3545s,  batch average loss: 1.1068\n",
      "Training  ------>   Batch count: 111/1073,  batch time: 0.3217s,  batch average loss: 1.1067\n",
      "Training  ------>   Batch count: 112/1073,  batch time: 0.2908s,  batch average loss: 1.1067\n",
      "Training  ------>   Batch count: 113/1073,  batch time: 0.3688s,  batch average loss: 1.1065\n",
      "Training  ------>   Batch count: 114/1073,  batch time: 0.2938s,  batch average loss: 1.1065\n",
      "Training  ------>   Batch count: 115/1073,  batch time: 0.4034s,  batch average loss: 1.1064\n",
      "Training  ------>   Batch count: 116/1073,  batch time: 0.3665s,  batch average loss: 1.1063\n",
      "Training  ------>   Batch count: 117/1073,  batch time: 0.3318s,  batch average loss: 1.1062\n",
      "Training  ------>   Batch count: 118/1073,  batch time: 0.3677s,  batch average loss: 1.1061\n",
      "Training  ------>   Batch count: 119/1073,  batch time: 0.2988s,  batch average loss: 1.1060\n",
      "Training  ------>   Batch count: 120/1073,  batch time: 0.3268s,  batch average loss: 1.1059\n",
      "Training  ------>   Batch count: 121/1073,  batch time: 0.3078s,  batch average loss: 1.1058\n",
      "Training  ------>   Batch count: 122/1073,  batch time: 0.3080s,  batch average loss: 1.1057\n",
      "Training  ------>   Batch count: 123/1073,  batch time: 0.3399s,  batch average loss: 1.1057\n",
      "Training  ------>   Batch count: 124/1073,  batch time: 0.3098s,  batch average loss: 1.1055\n",
      "Training  ------>   Batch count: 125/1073,  batch time: 0.3078s,  batch average loss: 1.1055\n",
      "Training  ------>   Batch count: 126/1073,  batch time: 0.3388s,  batch average loss: 1.1054\n",
      "Training  ------>   Batch count: 127/1073,  batch time: 0.3425s,  batch average loss: 1.1053\n",
      "Training  ------>   Batch count: 128/1073,  batch time: 0.2888s,  batch average loss: 1.1052\n",
      "Training  ------>   Batch count: 129/1073,  batch time: 0.3354s,  batch average loss: 1.1051\n",
      "Training  ------>   Batch count: 130/1073,  batch time: 0.2958s,  batch average loss: 1.1050\n",
      "Training  ------>   Batch count: 131/1073,  batch time: 0.3378s,  batch average loss: 1.1049\n",
      "Training  ------>   Batch count: 132/1073,  batch time: 0.3308s,  batch average loss: 1.1048\n",
      "Training  ------>   Batch count: 133/1073,  batch time: 0.2968s,  batch average loss: 1.1048\n",
      "Training  ------>   Batch count: 134/1073,  batch time: 0.3090s,  batch average loss: 1.1047\n",
      "Training  ------>   Batch count: 135/1073,  batch time: 0.3508s,  batch average loss: 1.1047\n",
      "Training  ------>   Batch count: 136/1073,  batch time: 0.4407s,  batch average loss: 1.1046\n",
      "Training  ------>   Batch count: 137/1073,  batch time: 0.2868s,  batch average loss: 1.1045\n",
      "Training  ------>   Batch count: 138/1073,  batch time: 0.2978s,  batch average loss: 1.1045\n",
      "Training  ------>   Batch count: 139/1073,  batch time: 0.3235s,  batch average loss: 1.1044\n",
      "Training  ------>   Batch count: 140/1073,  batch time: 0.3194s,  batch average loss: 1.1044\n",
      "Training  ------>   Batch count: 141/1073,  batch time: 0.3579s,  batch average loss: 1.1044\n",
      "Training  ------>   Batch count: 142/1073,  batch time: 0.2978s,  batch average loss: 1.1042\n",
      "Training  ------>   Batch count: 143/1073,  batch time: 0.3278s,  batch average loss: 1.1042\n",
      "Training  ------>   Batch count: 144/1073,  batch time: 0.3148s,  batch average loss: 1.1041\n",
      "Training  ------>   Batch count: 145/1073,  batch time: 0.3483s,  batch average loss: 1.1040\n",
      "Training  ------>   Batch count: 146/1073,  batch time: 0.3005s,  batch average loss: 1.1040\n",
      "Training  ------>   Batch count: 147/1073,  batch time: 0.3108s,  batch average loss: 1.1039\n",
      "Training  ------>   Batch count: 148/1073,  batch time: 0.3598s,  batch average loss: 1.1038\n",
      "Training  ------>   Batch count: 149/1073,  batch time: 0.2868s,  batch average loss: 1.1038\n",
      "Training  ------>   Batch count: 150/1073,  batch time: 0.3128s,  batch average loss: 1.1037\n",
      "Training  ------>   Batch count: 151/1073,  batch time: 0.3888s,  batch average loss: 1.1036\n",
      "Training  ------>   Batch count: 152/1073,  batch time: 0.3432s,  batch average loss: 1.1035\n",
      "Training  ------>   Batch count: 153/1073,  batch time: 0.3548s,  batch average loss: 1.1034\n",
      "Training  ------>   Batch count: 154/1073,  batch time: 0.3468s,  batch average loss: 1.1032\n",
      "Training  ------>   Batch count: 155/1073,  batch time: 0.3798s,  batch average loss: 1.1032\n",
      "Training  ------>   Batch count: 156/1073,  batch time: 0.3428s,  batch average loss: 1.1030\n",
      "Training  ------>   Batch count: 157/1073,  batch time: 0.3885s,  batch average loss: 1.1030\n",
      "Training  ------>   Batch count: 158/1073,  batch time: 0.3221s,  batch average loss: 1.1029\n",
      "Training  ------>   Batch count: 159/1073,  batch time: 0.3258s,  batch average loss: 1.1028\n",
      "Training  ------>   Batch count: 160/1073,  batch time: 0.3328s,  batch average loss: 1.1028\n",
      "Training  ------>   Batch count: 161/1073,  batch time: 0.3178s,  batch average loss: 1.1027\n",
      "Training  ------>   Batch count: 162/1073,  batch time: 0.3529s,  batch average loss: 1.1026\n",
      "Training  ------>   Batch count: 163/1073,  batch time: 0.3020s,  batch average loss: 1.1025\n",
      "Training  ------>   Batch count: 164/1073,  batch time: 0.3147s,  batch average loss: 1.1024\n",
      "Training  ------>   Batch count: 165/1073,  batch time: 0.3468s,  batch average loss: 1.1023\n",
      "Training  ------>   Batch count: 166/1073,  batch time: 0.3548s,  batch average loss: 1.1023\n",
      "Training  ------>   Batch count: 167/1073,  batch time: 0.3218s,  batch average loss: 1.1022\n",
      "Training  ------>   Batch count: 168/1073,  batch time: 0.3108s,  batch average loss: 1.1022\n",
      "Training  ------>   Batch count: 169/1073,  batch time: 0.2951s,  batch average loss: 1.1022\n",
      "Training  ------>   Batch count: 170/1073,  batch time: 0.2856s,  batch average loss: 1.1022\n",
      "Training  ------>   Batch count: 171/1073,  batch time: 0.3228s,  batch average loss: 1.1021\n",
      "Training  ------>   Batch count: 172/1073,  batch time: 0.3080s,  batch average loss: 1.1020\n",
      "Training  ------>   Batch count: 173/1073,  batch time: 0.3348s,  batch average loss: 1.1019\n",
      "Training  ------>   Batch count: 174/1073,  batch time: 0.2968s,  batch average loss: 1.1019\n",
      "Training  ------>   Batch count: 175/1073,  batch time: 0.3008s,  batch average loss: 1.1018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 176/1073,  batch time: 0.3093s,  batch average loss: 1.1017\n",
      "Training  ------>   Batch count: 177/1073,  batch time: 0.2878s,  batch average loss: 1.1016\n",
      "Training  ------>   Batch count: 178/1073,  batch time: 0.3558s,  batch average loss: 1.1016\n",
      "Training  ------>   Batch count: 179/1073,  batch time: 0.3428s,  batch average loss: 1.1015\n",
      "Training  ------>   Batch count: 180/1073,  batch time: 0.3388s,  batch average loss: 1.1015\n",
      "Training  ------>   Batch count: 181/1073,  batch time: 0.2892s,  batch average loss: 1.1014\n",
      "Training  ------>   Batch count: 182/1073,  batch time: 0.4042s,  batch average loss: 1.1013\n",
      "Training  ------>   Batch count: 183/1073,  batch time: 0.3241s,  batch average loss: 1.1013\n",
      "Training  ------>   Batch count: 184/1073,  batch time: 0.3288s,  batch average loss: 1.1012\n",
      "Training  ------>   Batch count: 185/1073,  batch time: 0.3092s,  batch average loss: 1.1012\n",
      "Training  ------>   Batch count: 186/1073,  batch time: 0.3608s,  batch average loss: 1.1011\n",
      "Training  ------>   Batch count: 187/1073,  batch time: 0.3087s,  batch average loss: 1.1010\n",
      "Training  ------>   Batch count: 188/1073,  batch time: 0.4097s,  batch average loss: 1.1010\n",
      "Training  ------>   Batch count: 189/1073,  batch time: 0.3117s,  batch average loss: 1.1009\n",
      "Training  ------>   Batch count: 190/1073,  batch time: 0.3747s,  batch average loss: 1.1008\n",
      "Training  ------>   Batch count: 191/1073,  batch time: 0.3314s,  batch average loss: 1.1008\n",
      "Training  ------>   Batch count: 192/1073,  batch time: 0.3316s,  batch average loss: 1.1007\n",
      "Training  ------>   Batch count: 193/1073,  batch time: 0.3279s,  batch average loss: 1.1006\n",
      "Training  ------>   Batch count: 194/1073,  batch time: 0.3047s,  batch average loss: 1.1005\n",
      "Training  ------>   Batch count: 195/1073,  batch time: 0.3688s,  batch average loss: 1.1004\n",
      "Training  ------>   Batch count: 196/1073,  batch time: 0.3317s,  batch average loss: 1.1003\n",
      "Training  ------>   Batch count: 197/1073,  batch time: 0.2637s,  batch average loss: 1.1002\n",
      "Training  ------>   Batch count: 198/1073,  batch time: 0.3687s,  batch average loss: 1.1001\n",
      "Training  ------>   Batch count: 199/1073,  batch time: 0.3005s,  batch average loss: 1.1001\n",
      "Training  ------>   Batch count: 200/1073,  batch time: 0.3417s,  batch average loss: 1.1000\n",
      "Training  ------>   Batch count: 201/1073,  batch time: 0.3189s,  batch average loss: 1.1000\n",
      "Training  ------>   Batch count: 202/1073,  batch time: 0.2765s,  batch average loss: 1.0999\n",
      "Training  ------>   Batch count: 203/1073,  batch time: 0.2880s,  batch average loss: 1.0998\n",
      "Training  ------>   Batch count: 204/1073,  batch time: 0.3259s,  batch average loss: 1.0997\n",
      "Training  ------>   Batch count: 205/1073,  batch time: 0.3323s,  batch average loss: 1.0996\n",
      "Training  ------>   Batch count: 206/1073,  batch time: 0.3230s,  batch average loss: 1.0995\n",
      "Training  ------>   Batch count: 207/1073,  batch time: 0.3397s,  batch average loss: 1.0995\n",
      "Training  ------>   Batch count: 208/1073,  batch time: 0.2938s,  batch average loss: 1.0994\n",
      "Training  ------>   Batch count: 209/1073,  batch time: 0.3437s,  batch average loss: 1.0994\n",
      "Training  ------>   Batch count: 210/1073,  batch time: 0.3497s,  batch average loss: 1.0993\n",
      "Training  ------>   Batch count: 211/1073,  batch time: 0.3417s,  batch average loss: 1.0992\n",
      "Training  ------>   Batch count: 212/1073,  batch time: 0.2948s,  batch average loss: 1.0991\n",
      "Training  ------>   Batch count: 213/1073,  batch time: 0.2992s,  batch average loss: 1.0990\n",
      "Training  ------>   Batch count: 214/1073,  batch time: 0.3312s,  batch average loss: 1.0989\n",
      "Training  ------>   Batch count: 215/1073,  batch time: 0.3054s,  batch average loss: 1.0988\n",
      "Training  ------>   Batch count: 216/1073,  batch time: 0.3312s,  batch average loss: 1.0987\n",
      "Training  ------>   Batch count: 217/1073,  batch time: 0.3776s,  batch average loss: 1.0986\n",
      "Training  ------>   Batch count: 218/1073,  batch time: 0.2805s,  batch average loss: 1.0986\n",
      "Training  ------>   Batch count: 219/1073,  batch time: 0.3279s,  batch average loss: 1.0985\n",
      "Training  ------>   Batch count: 220/1073,  batch time: 0.2990s,  batch average loss: 1.0984\n",
      "Training  ------>   Batch count: 221/1073,  batch time: 0.3354s,  batch average loss: 1.0983\n",
      "Training  ------>   Batch count: 222/1073,  batch time: 0.3041s,  batch average loss: 1.0982\n",
      "Training  ------>   Batch count: 223/1073,  batch time: 0.3428s,  batch average loss: 1.0982\n",
      "Training  ------>   Batch count: 224/1073,  batch time: 0.3999s,  batch average loss: 1.0981\n",
      "Training  ------>   Batch count: 225/1073,  batch time: 0.3020s,  batch average loss: 1.0980\n",
      "Training  ------>   Batch count: 226/1073,  batch time: 0.3507s,  batch average loss: 1.0979\n",
      "Training  ------>   Batch count: 227/1073,  batch time: 0.3351s,  batch average loss: 1.0978\n",
      "Training  ------>   Batch count: 228/1073,  batch time: 0.3330s,  batch average loss: 1.0977\n",
      "Training  ------>   Batch count: 229/1073,  batch time: 0.3518s,  batch average loss: 1.0976\n",
      "Training  ------>   Batch count: 230/1073,  batch time: 0.3160s,  batch average loss: 1.0975\n",
      "Training  ------>   Batch count: 231/1073,  batch time: 0.2949s,  batch average loss: 1.0974\n",
      "Training  ------>   Batch count: 232/1073,  batch time: 0.3479s,  batch average loss: 1.0974\n",
      "Training  ------>   Batch count: 233/1073,  batch time: 0.3839s,  batch average loss: 1.0973\n",
      "Training  ------>   Batch count: 234/1073,  batch time: 0.3231s,  batch average loss: 1.0972\n",
      "Training  ------>   Batch count: 235/1073,  batch time: 0.3380s,  batch average loss: 1.0973\n",
      "Training  ------>   Batch count: 236/1073,  batch time: 0.3197s,  batch average loss: 1.0972\n",
      "Training  ------>   Batch count: 237/1073,  batch time: 0.3392s,  batch average loss: 1.0972\n",
      "Training  ------>   Batch count: 238/1073,  batch time: 0.2994s,  batch average loss: 1.0971\n",
      "Training  ------>   Batch count: 239/1073,  batch time: 0.3794s,  batch average loss: 1.0970\n",
      "Training  ------>   Batch count: 240/1073,  batch time: 0.3397s,  batch average loss: 1.0969\n",
      "Training  ------>   Batch count: 241/1073,  batch time: 0.3849s,  batch average loss: 1.0968\n",
      "Training  ------>   Batch count: 242/1073,  batch time: 0.2930s,  batch average loss: 1.0968\n",
      "Training  ------>   Batch count: 243/1073,  batch time: 0.2949s,  batch average loss: 1.0967\n",
      "Training  ------>   Batch count: 244/1073,  batch time: 0.2826s,  batch average loss: 1.0966\n",
      "Training  ------>   Batch count: 245/1073,  batch time: 0.3500s,  batch average loss: 1.0965\n",
      "Training  ------>   Batch count: 246/1073,  batch time: 0.3210s,  batch average loss: 1.0964\n",
      "Training  ------>   Batch count: 247/1073,  batch time: 0.3189s,  batch average loss: 1.0963\n",
      "Training  ------>   Batch count: 248/1073,  batch time: 0.3624s,  batch average loss: 1.0962\n",
      "Training  ------>   Batch count: 249/1073,  batch time: 0.3990s,  batch average loss: 1.0961\n",
      "Training  ------>   Batch count: 250/1073,  batch time: 0.3345s,  batch average loss: 1.0961\n",
      "Training  ------>   Batch count: 251/1073,  batch time: 0.3459s,  batch average loss: 1.0959\n",
      "Training  ------>   Batch count: 252/1073,  batch time: 0.3108s,  batch average loss: 1.0958\n",
      "Training  ------>   Batch count: 253/1073,  batch time: 0.2983s,  batch average loss: 1.0958\n",
      "Training  ------>   Batch count: 254/1073,  batch time: 0.3337s,  batch average loss: 1.0957\n",
      "Training  ------>   Batch count: 255/1073,  batch time: 0.4094s,  batch average loss: 1.0957\n",
      "Training  ------>   Batch count: 256/1073,  batch time: 0.2840s,  batch average loss: 1.0957\n",
      "Training  ------>   Batch count: 257/1073,  batch time: 0.3704s,  batch average loss: 1.0956\n",
      "Training  ------>   Batch count: 258/1073,  batch time: 0.2659s,  batch average loss: 1.0955\n",
      "Training  ------>   Batch count: 259/1073,  batch time: 0.3164s,  batch average loss: 1.0955\n",
      "Training  ------>   Batch count: 260/1073,  batch time: 0.3604s,  batch average loss: 1.0954\n",
      "Training  ------>   Batch count: 261/1073,  batch time: 0.3448s,  batch average loss: 1.0954\n",
      "Training  ------>   Batch count: 262/1073,  batch time: 0.2870s,  batch average loss: 1.0953\n",
      "Training  ------>   Batch count: 263/1073,  batch time: 0.3034s,  batch average loss: 1.0952\n",
      "Training  ------>   Batch count: 264/1073,  batch time: 0.3173s,  batch average loss: 1.0951\n",
      "Training  ------>   Batch count: 265/1073,  batch time: 0.3210s,  batch average loss: 1.0951\n",
      "Training  ------>   Batch count: 266/1073,  batch time: 0.2919s,  batch average loss: 1.0950\n",
      "Training  ------>   Batch count: 267/1073,  batch time: 0.3603s,  batch average loss: 1.0949\n",
      "Training  ------>   Batch count: 268/1073,  batch time: 0.3431s,  batch average loss: 1.0948\n",
      "Training  ------>   Batch count: 269/1073,  batch time: 0.3325s,  batch average loss: 1.0947\n",
      "Training  ------>   Batch count: 270/1073,  batch time: 0.3033s,  batch average loss: 1.0946\n",
      "Training  ------>   Batch count: 271/1073,  batch time: 0.2879s,  batch average loss: 1.0945\n",
      "Training  ------>   Batch count: 272/1073,  batch time: 0.3340s,  batch average loss: 1.0944\n",
      "Training  ------>   Batch count: 273/1073,  batch time: 0.3164s,  batch average loss: 1.0943\n",
      "Training  ------>   Batch count: 274/1073,  batch time: 0.2831s,  batch average loss: 1.0942\n",
      "Training  ------>   Batch count: 275/1073,  batch time: 0.4051s,  batch average loss: 1.0941\n",
      "Training  ------>   Batch count: 276/1073,  batch time: 0.2920s,  batch average loss: 1.0940\n",
      "Training  ------>   Batch count: 277/1073,  batch time: 0.3431s,  batch average loss: 1.0939\n",
      "Training  ------>   Batch count: 278/1073,  batch time: 0.3125s,  batch average loss: 1.0939\n",
      "Training  ------>   Batch count: 279/1073,  batch time: 0.3302s,  batch average loss: 1.0939\n",
      "Training  ------>   Batch count: 280/1073,  batch time: 0.3060s,  batch average loss: 1.0937\n",
      "Training  ------>   Batch count: 281/1073,  batch time: 0.2768s,  batch average loss: 1.0936\n",
      "Training  ------>   Batch count: 282/1073,  batch time: 0.3357s,  batch average loss: 1.0935\n",
      "Training  ------>   Batch count: 283/1073,  batch time: 0.4245s,  batch average loss: 1.0934\n",
      "Training  ------>   Batch count: 284/1073,  batch time: 0.2812s,  batch average loss: 1.0933\n",
      "Training  ------>   Batch count: 285/1073,  batch time: 0.3146s,  batch average loss: 1.0932\n",
      "Training  ------>   Batch count: 286/1073,  batch time: 0.3125s,  batch average loss: 1.0931\n",
      "Training  ------>   Batch count: 287/1073,  batch time: 0.2924s,  batch average loss: 1.0929\n",
      "Training  ------>   Batch count: 288/1073,  batch time: 0.3502s,  batch average loss: 1.0928\n",
      "Training  ------>   Batch count: 289/1073,  batch time: 0.2812s,  batch average loss: 1.0926\n",
      "Training  ------>   Batch count: 290/1073,  batch time: 0.3141s,  batch average loss: 1.0925\n",
      "Training  ------>   Batch count: 291/1073,  batch time: 0.3030s,  batch average loss: 1.0925\n",
      "Training  ------>   Batch count: 292/1073,  batch time: 0.3199s,  batch average loss: 1.0924\n",
      "Training  ------>   Batch count: 293/1073,  batch time: 0.3043s,  batch average loss: 1.0924\n",
      "Training  ------>   Batch count: 294/1073,  batch time: 0.3259s,  batch average loss: 1.0922\n",
      "Training  ------>   Batch count: 295/1073,  batch time: 0.3575s,  batch average loss: 1.0922\n",
      "Training  ------>   Batch count: 296/1073,  batch time: 0.3166s,  batch average loss: 1.0921\n",
      "Training  ------>   Batch count: 297/1073,  batch time: 0.3129s,  batch average loss: 1.0921\n",
      "Training  ------>   Batch count: 298/1073,  batch time: 0.2945s,  batch average loss: 1.0920\n",
      "Training  ------>   Batch count: 299/1073,  batch time: 0.2923s,  batch average loss: 1.0919\n",
      "Training  ------>   Batch count: 300/1073,  batch time: 0.3036s,  batch average loss: 1.0918\n",
      "Training  ------>   Batch count: 301/1073,  batch time: 0.3469s,  batch average loss: 1.0917\n",
      "Training  ------>   Batch count: 302/1073,  batch time: 0.2982s,  batch average loss: 1.0916\n",
      "Training  ------>   Batch count: 303/1073,  batch time: 0.3105s,  batch average loss: 1.0915\n",
      "Training  ------>   Batch count: 304/1073,  batch time: 0.3281s,  batch average loss: 1.0915\n",
      "Training  ------>   Batch count: 305/1073,  batch time: 0.3568s,  batch average loss: 1.0913\n",
      "Training  ------>   Batch count: 306/1073,  batch time: 0.3066s,  batch average loss: 1.0912\n",
      "Training  ------>   Batch count: 307/1073,  batch time: 0.3025s,  batch average loss: 1.0911\n",
      "Training  ------>   Batch count: 308/1073,  batch time: 0.2946s,  batch average loss: 1.0910\n",
      "Training  ------>   Batch count: 309/1073,  batch time: 0.4083s,  batch average loss: 1.0909\n",
      "Training  ------>   Batch count: 310/1073,  batch time: 0.2869s,  batch average loss: 1.0908\n",
      "Training  ------>   Batch count: 311/1073,  batch time: 0.3033s,  batch average loss: 1.0906\n",
      "Training  ------>   Batch count: 312/1073,  batch time: 0.3119s,  batch average loss: 1.0905\n",
      "Training  ------>   Batch count: 313/1073,  batch time: 0.3250s,  batch average loss: 1.0905\n",
      "Training  ------>   Batch count: 314/1073,  batch time: 0.3272s,  batch average loss: 1.0904\n",
      "Training  ------>   Batch count: 315/1073,  batch time: 0.3125s,  batch average loss: 1.0903\n",
      "Training  ------>   Batch count: 316/1073,  batch time: 0.2974s,  batch average loss: 1.0902\n",
      "Training  ------>   Batch count: 317/1073,  batch time: 0.2969s,  batch average loss: 1.0902\n",
      "Training  ------>   Batch count: 318/1073,  batch time: 0.3078s,  batch average loss: 1.0901\n",
      "Training  ------>   Batch count: 319/1073,  batch time: 0.3335s,  batch average loss: 1.0900\n",
      "Training  ------>   Batch count: 320/1073,  batch time: 0.3089s,  batch average loss: 1.0899\n",
      "Training  ------>   Batch count: 321/1073,  batch time: 0.3569s,  batch average loss: 1.0898\n",
      "Training  ------>   Batch count: 322/1073,  batch time: 0.2759s,  batch average loss: 1.0898\n",
      "Training  ------>   Batch count: 323/1073,  batch time: 0.3350s,  batch average loss: 1.0897\n",
      "Training  ------>   Batch count: 324/1073,  batch time: 0.3825s,  batch average loss: 1.0896\n",
      "Training  ------>   Batch count: 325/1073,  batch time: 0.2948s,  batch average loss: 1.0895\n",
      "Training  ------>   Batch count: 326/1073,  batch time: 0.3397s,  batch average loss: 1.0893\n",
      "Training  ------>   Batch count: 327/1073,  batch time: 0.3622s,  batch average loss: 1.0892\n",
      "Training  ------>   Batch count: 328/1073,  batch time: 0.3191s,  batch average loss: 1.0892\n",
      "Training  ------>   Batch count: 329/1073,  batch time: 0.3438s,  batch average loss: 1.0891\n",
      "Training  ------>   Batch count: 330/1073,  batch time: 0.3110s,  batch average loss: 1.0890\n",
      "Training  ------>   Batch count: 331/1073,  batch time: 0.2926s,  batch average loss: 1.0889\n",
      "Training  ------>   Batch count: 332/1073,  batch time: 0.3351s,  batch average loss: 1.0889\n",
      "Training  ------>   Batch count: 333/1073,  batch time: 0.3485s,  batch average loss: 1.0888\n",
      "Training  ------>   Batch count: 334/1073,  batch time: 0.2756s,  batch average loss: 1.0887\n",
      "Training  ------>   Batch count: 335/1073,  batch time: 0.3385s,  batch average loss: 1.0886\n",
      "Training  ------>   Batch count: 336/1073,  batch time: 0.3115s,  batch average loss: 1.0885\n",
      "Training  ------>   Batch count: 337/1073,  batch time: 0.3072s,  batch average loss: 1.0884\n",
      "Training  ------>   Batch count: 338/1073,  batch time: 0.2954s,  batch average loss: 1.0884\n",
      "Training  ------>   Batch count: 339/1073,  batch time: 0.2972s,  batch average loss: 1.0883\n",
      "Training  ------>   Batch count: 340/1073,  batch time: 0.3597s,  batch average loss: 1.0882\n",
      "Training  ------>   Batch count: 341/1073,  batch time: 0.3518s,  batch average loss: 1.0882\n",
      "Training  ------>   Batch count: 342/1073,  batch time: 0.3318s,  batch average loss: 1.0882\n",
      "Training  ------>   Batch count: 343/1073,  batch time: 0.2732s,  batch average loss: 1.0881\n",
      "Training  ------>   Batch count: 344/1073,  batch time: 0.3938s,  batch average loss: 1.0880\n",
      "Training  ------>   Batch count: 345/1073,  batch time: 0.3438s,  batch average loss: 1.0880\n",
      "Training  ------>   Batch count: 346/1073,  batch time: 0.3368s,  batch average loss: 1.0878\n",
      "Training  ------>   Batch count: 347/1073,  batch time: 0.3948s,  batch average loss: 1.0877\n",
      "Training  ------>   Batch count: 348/1073,  batch time: 0.3057s,  batch average loss: 1.0876\n",
      "Training  ------>   Batch count: 349/1073,  batch time: 0.3574s,  batch average loss: 1.0875\n",
      "Training  ------>   Batch count: 350/1073,  batch time: 0.3078s,  batch average loss: 1.0875\n",
      "Training  ------>   Batch count: 351/1073,  batch time: 0.3506s,  batch average loss: 1.0874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 352/1073,  batch time: 0.3248s,  batch average loss: 1.0873\n",
      "Training  ------>   Batch count: 353/1073,  batch time: 0.3268s,  batch average loss: 1.0872\n",
      "Training  ------>   Batch count: 354/1073,  batch time: 0.3602s,  batch average loss: 1.0871\n",
      "Training  ------>   Batch count: 355/1073,  batch time: 0.3258s,  batch average loss: 1.0870\n",
      "Training  ------>   Batch count: 356/1073,  batch time: 0.3608s,  batch average loss: 1.0869\n",
      "Training  ------>   Batch count: 357/1073,  batch time: 0.3038s,  batch average loss: 1.0868\n",
      "Training  ------>   Batch count: 358/1073,  batch time: 0.3358s,  batch average loss: 1.0868\n",
      "Training  ------>   Batch count: 359/1073,  batch time: 0.3088s,  batch average loss: 1.0866\n",
      "Training  ------>   Batch count: 360/1073,  batch time: 0.2771s,  batch average loss: 1.0866\n",
      "Training  ------>   Batch count: 361/1073,  batch time: 0.3808s,  batch average loss: 1.0865\n",
      "Training  ------>   Batch count: 362/1073,  batch time: 0.3378s,  batch average loss: 1.0865\n",
      "Training  ------>   Batch count: 363/1073,  batch time: 0.3128s,  batch average loss: 1.0864\n",
      "Training  ------>   Batch count: 364/1073,  batch time: 0.3728s,  batch average loss: 1.0863\n",
      "Training  ------>   Batch count: 365/1073,  batch time: 0.3158s,  batch average loss: 1.0862\n",
      "Training  ------>   Batch count: 366/1073,  batch time: 0.3161s,  batch average loss: 1.0861\n",
      "Training  ------>   Batch count: 367/1073,  batch time: 0.3068s,  batch average loss: 1.0861\n",
      "Training  ------>   Batch count: 368/1073,  batch time: 0.3098s,  batch average loss: 1.0861\n",
      "Training  ------>   Batch count: 369/1073,  batch time: 0.3644s,  batch average loss: 1.0860\n",
      "Training  ------>   Batch count: 370/1073,  batch time: 0.3208s,  batch average loss: 1.0859\n",
      "Training  ------>   Batch count: 371/1073,  batch time: 0.3598s,  batch average loss: 1.0858\n",
      "Training  ------>   Batch count: 372/1073,  batch time: 0.2724s,  batch average loss: 1.0857\n",
      "Training  ------>   Batch count: 373/1073,  batch time: 0.2988s,  batch average loss: 1.0856\n",
      "Training  ------>   Batch count: 374/1073,  batch time: 0.3018s,  batch average loss: 1.0855\n",
      "Training  ------>   Batch count: 375/1073,  batch time: 0.3166s,  batch average loss: 1.0854\n",
      "Training  ------>   Batch count: 376/1073,  batch time: 0.3028s,  batch average loss: 1.0854\n",
      "Training  ------>   Batch count: 377/1073,  batch time: 0.4138s,  batch average loss: 1.0853\n",
      "Training  ------>   Batch count: 378/1073,  batch time: 0.3056s,  batch average loss: 1.0852\n",
      "Training  ------>   Batch count: 379/1073,  batch time: 0.3428s,  batch average loss: 1.0851\n",
      "Training  ------>   Batch count: 380/1073,  batch time: 0.2908s,  batch average loss: 1.0850\n",
      "Training  ------>   Batch count: 381/1073,  batch time: 0.3341s,  batch average loss: 1.0849\n",
      "Training  ------>   Batch count: 382/1073,  batch time: 0.3028s,  batch average loss: 1.0849\n",
      "Training  ------>   Batch count: 383/1073,  batch time: 0.3088s,  batch average loss: 1.0848\n",
      "Training  ------>   Batch count: 384/1073,  batch time: 0.4267s,  batch average loss: 1.0848\n",
      "Training  ------>   Batch count: 385/1073,  batch time: 0.3108s,  batch average loss: 1.0847\n",
      "Training  ------>   Batch count: 386/1073,  batch time: 0.3268s,  batch average loss: 1.0846\n",
      "Training  ------>   Batch count: 387/1073,  batch time: 0.3578s,  batch average loss: 1.0845\n",
      "Training  ------>   Batch count: 388/1073,  batch time: 0.2948s,  batch average loss: 1.0845\n",
      "Training  ------>   Batch count: 389/1073,  batch time: 0.3638s,  batch average loss: 1.0844\n",
      "Training  ------>   Batch count: 390/1073,  batch time: 0.3816s,  batch average loss: 1.0843\n",
      "Training  ------>   Batch count: 391/1073,  batch time: 0.3048s,  batch average loss: 1.0842\n",
      "Training  ------>   Batch count: 392/1073,  batch time: 0.3828s,  batch average loss: 1.0841\n",
      "Training  ------>   Batch count: 393/1073,  batch time: 0.4192s,  batch average loss: 1.0841\n",
      "Training  ------>   Batch count: 394/1073,  batch time: 0.3950s,  batch average loss: 1.0840\n",
      "Training  ------>   Batch count: 395/1073,  batch time: 0.3006s,  batch average loss: 1.0840\n",
      "Training  ------>   Batch count: 396/1073,  batch time: 0.3150s,  batch average loss: 1.0839\n",
      "Training  ------>   Batch count: 397/1073,  batch time: 0.4133s,  batch average loss: 1.0838\n",
      "Training  ------>   Batch count: 398/1073,  batch time: 0.3215s,  batch average loss: 1.0838\n",
      "Training  ------>   Batch count: 399/1073,  batch time: 0.3247s,  batch average loss: 1.0837\n",
      "Training  ------>   Batch count: 400/1073,  batch time: 0.3135s,  batch average loss: 1.0836\n",
      "Training  ------>   Batch count: 401/1073,  batch time: 0.3641s,  batch average loss: 1.0836\n",
      "Training  ------>   Batch count: 402/1073,  batch time: 0.3015s,  batch average loss: 1.0835\n",
      "Training  ------>   Batch count: 403/1073,  batch time: 0.3139s,  batch average loss: 1.0834\n",
      "Training  ------>   Batch count: 404/1073,  batch time: 0.2716s,  batch average loss: 1.0833\n",
      "Training  ------>   Batch count: 405/1073,  batch time: 0.3588s,  batch average loss: 1.0832\n",
      "Training  ------>   Batch count: 406/1073,  batch time: 0.4033s,  batch average loss: 1.0831\n",
      "Training  ------>   Batch count: 407/1073,  batch time: 0.3594s,  batch average loss: 1.0831\n",
      "Training  ------>   Batch count: 408/1073,  batch time: 0.2969s,  batch average loss: 1.0830\n",
      "Training  ------>   Batch count: 409/1073,  batch time: 0.2969s,  batch average loss: 1.0829\n",
      "Training  ------>   Batch count: 410/1073,  batch time: 0.3593s,  batch average loss: 1.0828\n",
      "Training  ------>   Batch count: 411/1073,  batch time: 0.2969s,  batch average loss: 1.0827\n",
      "Training  ------>   Batch count: 412/1073,  batch time: 0.3081s,  batch average loss: 1.0826\n",
      "Training  ------>   Batch count: 413/1073,  batch time: 0.3437s,  batch average loss: 1.0826\n",
      "Training  ------>   Batch count: 414/1073,  batch time: 0.3437s,  batch average loss: 1.0824\n",
      "Training  ------>   Batch count: 415/1073,  batch time: 0.2712s,  batch average loss: 1.0823\n",
      "Training  ------>   Batch count: 416/1073,  batch time: 0.3437s,  batch average loss: 1.0822\n",
      "Training  ------>   Batch count: 417/1073,  batch time: 0.3082s,  batch average loss: 1.0822\n",
      "Training  ------>   Batch count: 418/1073,  batch time: 0.3393s,  batch average loss: 1.0821\n",
      "Training  ------>   Batch count: 419/1073,  batch time: 0.3043s,  batch average loss: 1.0820\n",
      "Training  ------>   Batch count: 420/1073,  batch time: 0.3424s,  batch average loss: 1.0819\n",
      "Training  ------>   Batch count: 421/1073,  batch time: 0.3564s,  batch average loss: 1.0818\n",
      "Training  ------>   Batch count: 422/1073,  batch time: 0.3298s,  batch average loss: 1.0818\n",
      "Training  ------>   Batch count: 423/1073,  batch time: 0.2985s,  batch average loss: 1.0817\n",
      "Training  ------>   Batch count: 424/1073,  batch time: 0.2921s,  batch average loss: 1.0816\n",
      "Training  ------>   Batch count: 425/1073,  batch time: 0.4520s,  batch average loss: 1.0815\n",
      "Training  ------>   Batch count: 426/1073,  batch time: 0.3206s,  batch average loss: 1.0814\n",
      "Training  ------>   Batch count: 427/1073,  batch time: 0.4066s,  batch average loss: 1.0813\n",
      "Training  ------>   Batch count: 428/1073,  batch time: 0.3145s,  batch average loss: 1.0812\n",
      "Training  ------>   Batch count: 429/1073,  batch time: 0.2809s,  batch average loss: 1.0811\n",
      "Training  ------>   Batch count: 430/1073,  batch time: 0.3591s,  batch average loss: 1.0810\n",
      "Training  ------>   Batch count: 431/1073,  batch time: 0.3147s,  batch average loss: 1.0809\n",
      "Training  ------>   Batch count: 432/1073,  batch time: 0.2641s,  batch average loss: 1.0808\n",
      "Training  ------>   Batch count: 433/1073,  batch time: 0.3125s,  batch average loss: 1.0807\n",
      "Training  ------>   Batch count: 434/1073,  batch time: 0.3281s,  batch average loss: 1.0806\n",
      "Training  ------>   Batch count: 435/1073,  batch time: 0.3332s,  batch average loss: 1.0806\n",
      "Training  ------>   Batch count: 436/1073,  batch time: 0.3125s,  batch average loss: 1.0805\n",
      "Training  ------>   Batch count: 437/1073,  batch time: 0.3437s,  batch average loss: 1.0804\n",
      "Training  ------>   Batch count: 438/1073,  batch time: 0.3281s,  batch average loss: 1.0803\n",
      "Training  ------>   Batch count: 439/1073,  batch time: 0.2969s,  batch average loss: 1.0802\n",
      "Training  ------>   Batch count: 440/1073,  batch time: 0.2796s,  batch average loss: 1.0801\n",
      "Training  ------>   Batch count: 441/1073,  batch time: 0.3125s,  batch average loss: 1.0801\n",
      "Training  ------>   Batch count: 442/1073,  batch time: 0.3080s,  batch average loss: 1.0801\n",
      "Training  ------>   Batch count: 443/1073,  batch time: 0.3125s,  batch average loss: 1.0800\n",
      "Training  ------>   Batch count: 444/1073,  batch time: 0.4375s,  batch average loss: 1.0799\n",
      "Training  ------>   Batch count: 445/1073,  batch time: 0.3281s,  batch average loss: 1.0798\n",
      "Training  ------>   Batch count: 446/1073,  batch time: 0.3281s,  batch average loss: 1.0797\n",
      "Training  ------>   Batch count: 447/1073,  batch time: 0.3861s,  batch average loss: 1.0796\n",
      "Training  ------>   Batch count: 448/1073,  batch time: 0.3125s,  batch average loss: 1.0795\n",
      "Training  ------>   Batch count: 449/1073,  batch time: 0.3437s,  batch average loss: 1.0794\n",
      "Training  ------>   Batch count: 450/1073,  batch time: 0.3750s,  batch average loss: 1.0793\n",
      "Training  ------>   Batch count: 451/1073,  batch time: 0.3125s,  batch average loss: 1.0793\n",
      "Training  ------>   Batch count: 452/1073,  batch time: 0.2969s,  batch average loss: 1.0792\n",
      "Training  ------>   Batch count: 453/1073,  batch time: 0.2924s,  batch average loss: 1.0791\n",
      "Training  ------>   Batch count: 454/1073,  batch time: 0.3125s,  batch average loss: 1.0790\n",
      "Training  ------>   Batch count: 455/1073,  batch time: 0.3437s,  batch average loss: 1.0789\n",
      "Training  ------>   Batch count: 456/1073,  batch time: 0.2969s,  batch average loss: 1.0789\n",
      "Training  ------>   Batch count: 457/1073,  batch time: 0.4062s,  batch average loss: 1.0787\n",
      "Training  ------>   Batch count: 458/1073,  batch time: 0.2812s,  batch average loss: 1.0787\n",
      "Training  ------>   Batch count: 459/1073,  batch time: 0.3862s,  batch average loss: 1.0786\n",
      "Training  ------>   Batch count: 460/1073,  batch time: 0.3281s,  batch average loss: 1.0785\n",
      "Training  ------>   Batch count: 461/1073,  batch time: 0.2969s,  batch average loss: 1.0784\n",
      "Training  ------>   Batch count: 462/1073,  batch time: 0.3281s,  batch average loss: 1.0784\n",
      "Training  ------>   Batch count: 463/1073,  batch time: 0.3281s,  batch average loss: 1.0783\n",
      "Training  ------>   Batch count: 464/1073,  batch time: 0.3125s,  batch average loss: 1.0782\n",
      "Training  ------>   Batch count: 465/1073,  batch time: 0.3247s,  batch average loss: 1.0782\n",
      "Training  ------>   Batch count: 466/1073,  batch time: 0.2969s,  batch average loss: 1.0781\n",
      "Training  ------>   Batch count: 467/1073,  batch time: 0.2969s,  batch average loss: 1.0780\n",
      "Training  ------>   Batch count: 468/1073,  batch time: 0.2969s,  batch average loss: 1.0779\n",
      "Training  ------>   Batch count: 469/1073,  batch time: 0.3125s,  batch average loss: 1.0778\n",
      "Training  ------>   Batch count: 470/1073,  batch time: 0.3125s,  batch average loss: 1.0777\n",
      "Training  ------>   Batch count: 471/1073,  batch time: 0.3281s,  batch average loss: 1.0777\n",
      "Training  ------>   Batch count: 472/1073,  batch time: 0.2726s,  batch average loss: 1.0776\n",
      "Training  ------>   Batch count: 473/1073,  batch time: 0.3125s,  batch average loss: 1.0775\n",
      "Training  ------>   Batch count: 474/1073,  batch time: 0.3125s,  batch average loss: 1.0774\n",
      "Training  ------>   Batch count: 475/1073,  batch time: 0.3281s,  batch average loss: 1.0773\n",
      "Training  ------>   Batch count: 476/1073,  batch time: 0.2738s,  batch average loss: 1.0773\n",
      "Training  ------>   Batch count: 477/1073,  batch time: 0.3281s,  batch average loss: 1.0772\n",
      "Training  ------>   Batch count: 478/1073,  batch time: 0.3078s,  batch average loss: 1.0771\n",
      "Training  ------>   Batch count: 479/1073,  batch time: 0.3760s,  batch average loss: 1.0770\n",
      "Training  ------>   Batch count: 480/1073,  batch time: 0.3750s,  batch average loss: 1.0770\n",
      "Training  ------>   Batch count: 481/1073,  batch time: 0.2968s,  batch average loss: 1.0769\n",
      "Training  ------>   Batch count: 482/1073,  batch time: 0.2812s,  batch average loss: 1.0768\n",
      "Training  ------>   Batch count: 483/1073,  batch time: 0.2969s,  batch average loss: 1.0767\n",
      "Training  ------>   Batch count: 484/1073,  batch time: 0.2767s,  batch average loss: 1.0766\n",
      "Training  ------>   Batch count: 485/1073,  batch time: 0.3437s,  batch average loss: 1.0765\n",
      "Training  ------>   Batch count: 486/1073,  batch time: 0.3125s,  batch average loss: 1.0764\n",
      "Training  ------>   Batch count: 487/1073,  batch time: 0.3125s,  batch average loss: 1.0763\n",
      "Training  ------>   Batch count: 488/1073,  batch time: 0.3125s,  batch average loss: 1.0763\n",
      "Training  ------>   Batch count: 489/1073,  batch time: 0.3281s,  batch average loss: 1.0762\n",
      "Training  ------>   Batch count: 490/1073,  batch time: 0.3090s,  batch average loss: 1.0761\n",
      "Training  ------>   Batch count: 491/1073,  batch time: 0.3281s,  batch average loss: 1.0761\n",
      "Training  ------>   Batch count: 492/1073,  batch time: 0.2969s,  batch average loss: 1.0760\n",
      "Training  ------>   Batch count: 493/1073,  batch time: 0.3281s,  batch average loss: 1.0759\n",
      "Training  ------>   Batch count: 494/1073,  batch time: 0.3594s,  batch average loss: 1.0758\n",
      "Training  ------>   Batch count: 495/1073,  batch time: 0.3125s,  batch average loss: 1.0758\n",
      "Training  ------>   Batch count: 496/1073,  batch time: 0.3393s,  batch average loss: 1.0757\n",
      "Training  ------>   Batch count: 497/1073,  batch time: 0.3108s,  batch average loss: 1.0757\n",
      "Training  ------>   Batch count: 498/1073,  batch time: 0.2969s,  batch average loss: 1.0756\n",
      "Training  ------>   Batch count: 499/1073,  batch time: 0.3125s,  batch average loss: 1.0754\n",
      "Training  ------>   Batch count: 500/1073,  batch time: 0.2981s,  batch average loss: 1.0754\n",
      "Training  ------>   Batch count: 501/1073,  batch time: 0.3125s,  batch average loss: 1.0753\n",
      "Training  ------>   Batch count: 502/1073,  batch time: 0.3236s,  batch average loss: 1.0752\n",
      "Training  ------>   Batch count: 503/1073,  batch time: 0.2969s,  batch average loss: 1.0751\n",
      "Training  ------>   Batch count: 504/1073,  batch time: 0.3437s,  batch average loss: 1.0750\n",
      "Training  ------>   Batch count: 505/1073,  batch time: 0.2968s,  batch average loss: 1.0750\n",
      "Training  ------>   Batch count: 506/1073,  batch time: 0.3291s,  batch average loss: 1.0749\n",
      "Training  ------>   Batch count: 507/1073,  batch time: 0.3281s,  batch average loss: 1.0748\n",
      "Training  ------>   Batch count: 508/1073,  batch time: 0.2933s,  batch average loss: 1.0747\n",
      "Training  ------>   Batch count: 509/1073,  batch time: 0.3281s,  batch average loss: 1.0746\n",
      "Training  ------>   Batch count: 510/1073,  batch time: 0.3281s,  batch average loss: 1.0745\n",
      "Training  ------>   Batch count: 511/1073,  batch time: 0.4062s,  batch average loss: 1.0745\n",
      "Training  ------>   Batch count: 512/1073,  batch time: 0.2969s,  batch average loss: 1.0744\n",
      "Training  ------>   Batch count: 513/1073,  batch time: 0.3281s,  batch average loss: 1.0744\n",
      "Training  ------>   Batch count: 514/1073,  batch time: 0.3080s,  batch average loss: 1.0743\n",
      "Training  ------>   Batch count: 515/1073,  batch time: 0.3437s,  batch average loss: 1.0743\n",
      "Training  ------>   Batch count: 516/1073,  batch time: 0.2812s,  batch average loss: 1.0742\n",
      "Training  ------>   Batch count: 517/1073,  batch time: 0.2969s,  batch average loss: 1.0742\n",
      "Training  ------>   Batch count: 518/1073,  batch time: 0.2968s,  batch average loss: 1.0741\n",
      "Training  ------>   Batch count: 519/1073,  batch time: 0.3292s,  batch average loss: 1.0740\n",
      "Training  ------>   Batch count: 520/1073,  batch time: 0.3386s,  batch average loss: 1.0739\n",
      "Training  ------>   Batch count: 521/1073,  batch time: 0.3437s,  batch average loss: 1.0738\n",
      "Training  ------>   Batch count: 522/1073,  batch time: 0.3125s,  batch average loss: 1.0738\n",
      "Training  ------>   Batch count: 523/1073,  batch time: 0.3750s,  batch average loss: 1.0737\n",
      "Training  ------>   Batch count: 524/1073,  batch time: 0.2973s,  batch average loss: 1.0736\n",
      "Training  ------>   Batch count: 525/1073,  batch time: 0.3293s,  batch average loss: 1.0734\n",
      "Training  ------>   Batch count: 526/1073,  batch time: 0.3393s,  batch average loss: 1.0734\n",
      "Training  ------>   Batch count: 527/1073,  batch time: 0.2958s,  batch average loss: 1.0733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 528/1073,  batch time: 0.2969s,  batch average loss: 1.0732\n",
      "Training  ------>   Batch count: 529/1073,  batch time: 0.3125s,  batch average loss: 1.0731\n",
      "Training  ------>   Batch count: 530/1073,  batch time: 0.3750s,  batch average loss: 1.0731\n",
      "Training  ------>   Batch count: 531/1073,  batch time: 0.3437s,  batch average loss: 1.0730\n",
      "Training  ------>   Batch count: 532/1073,  batch time: 0.3393s,  batch average loss: 1.0729\n",
      "Training  ------>   Batch count: 533/1073,  batch time: 0.3594s,  batch average loss: 1.0729\n",
      "Training  ------>   Batch count: 534/1073,  batch time: 0.3125s,  batch average loss: 1.0728\n",
      "Training  ------>   Batch count: 535/1073,  batch time: 0.3125s,  batch average loss: 1.0728\n",
      "Training  ------>   Batch count: 536/1073,  batch time: 0.2968s,  batch average loss: 1.0727\n",
      "Training  ------>   Batch count: 537/1073,  batch time: 0.4375s,  batch average loss: 1.0726\n",
      "Training  ------>   Batch count: 538/1073,  batch time: 0.3549s,  batch average loss: 1.0726\n",
      "Training  ------>   Batch count: 539/1073,  batch time: 0.2969s,  batch average loss: 1.0725\n",
      "Training  ------>   Batch count: 540/1073,  batch time: 0.3125s,  batch average loss: 1.0724\n",
      "Training  ------>   Batch count: 541/1073,  batch time: 0.3125s,  batch average loss: 1.0724\n",
      "Training  ------>   Batch count: 542/1073,  batch time: 0.2969s,  batch average loss: 1.0723\n",
      "Training  ------>   Batch count: 543/1073,  batch time: 0.3906s,  batch average loss: 1.0722\n",
      "Training  ------>   Batch count: 544/1073,  batch time: 0.2778s,  batch average loss: 1.0722\n",
      "Training  ------>   Batch count: 545/1073,  batch time: 0.3437s,  batch average loss: 1.0721\n",
      "Training  ------>   Batch count: 546/1073,  batch time: 0.3593s,  batch average loss: 1.0720\n",
      "Training  ------>   Batch count: 547/1073,  batch time: 0.3281s,  batch average loss: 1.0719\n",
      "Training  ------>   Batch count: 548/1073,  batch time: 0.2969s,  batch average loss: 1.0718\n",
      "Training  ------>   Batch count: 549/1073,  batch time: 0.3125s,  batch average loss: 1.0717\n",
      "Training  ------>   Batch count: 550/1073,  batch time: 0.3549s,  batch average loss: 1.0717\n",
      "Training  ------>   Batch count: 551/1073,  batch time: 0.2812s,  batch average loss: 1.0716\n",
      "Training  ------>   Batch count: 552/1073,  batch time: 0.2968s,  batch average loss: 1.0715\n",
      "Training  ------>   Batch count: 553/1073,  batch time: 0.3125s,  batch average loss: 1.0714\n",
      "Training  ------>   Batch count: 554/1073,  batch time: 0.3281s,  batch average loss: 1.0713\n",
      "Training  ------>   Batch count: 555/1073,  batch time: 0.2969s,  batch average loss: 1.0712\n",
      "Training  ------>   Batch count: 556/1073,  batch time: 0.3403s,  batch average loss: 1.0712\n",
      "Training  ------>   Batch count: 557/1073,  batch time: 0.3125s,  batch average loss: 1.0711\n",
      "Training  ------>   Batch count: 558/1073,  batch time: 0.3594s,  batch average loss: 1.0710\n",
      "Training  ------>   Batch count: 559/1073,  batch time: 0.3593s,  batch average loss: 1.0709\n",
      "Training  ------>   Batch count: 560/1073,  batch time: 0.3437s,  batch average loss: 1.0708\n",
      "Training  ------>   Batch count: 561/1073,  batch time: 0.3125s,  batch average loss: 1.0707\n",
      "Training  ------>   Batch count: 562/1073,  batch time: 0.3549s,  batch average loss: 1.0706\n",
      "Training  ------>   Batch count: 563/1073,  batch time: 0.3232s,  batch average loss: 1.0705\n",
      "Training  ------>   Batch count: 564/1073,  batch time: 0.3281s,  batch average loss: 1.0704\n",
      "Training  ------>   Batch count: 565/1073,  batch time: 0.3437s,  batch average loss: 1.0703\n",
      "Training  ------>   Batch count: 566/1073,  batch time: 0.2969s,  batch average loss: 1.0703\n",
      "Training  ------>   Batch count: 567/1073,  batch time: 0.3125s,  batch average loss: 1.0702\n",
      "Training  ------>   Batch count: 568/1073,  batch time: 0.2924s,  batch average loss: 1.0702\n",
      "Training  ------>   Batch count: 569/1073,  batch time: 0.3281s,  batch average loss: 1.0700\n",
      "Training  ------>   Batch count: 570/1073,  batch time: 0.2969s,  batch average loss: 1.0700\n",
      "Training  ------>   Batch count: 571/1073,  batch time: 0.3906s,  batch average loss: 1.0699\n",
      "Training  ------>   Batch count: 572/1073,  batch time: 0.3437s,  batch average loss: 1.0698\n",
      "Training  ------>   Batch count: 573/1073,  batch time: 0.3281s,  batch average loss: 1.0697\n",
      "Training  ------>   Batch count: 574/1073,  batch time: 0.3080s,  batch average loss: 1.0696\n",
      "Training  ------>   Batch count: 575/1073,  batch time: 0.3342s,  batch average loss: 1.0695\n",
      "Training  ------>   Batch count: 576/1073,  batch time: 0.4218s,  batch average loss: 1.0695\n",
      "Training  ------>   Batch count: 577/1073,  batch time: 0.2969s,  batch average loss: 1.0694\n",
      "Training  ------>   Batch count: 578/1073,  batch time: 0.3281s,  batch average loss: 1.0694\n",
      "Training  ------>   Batch count: 579/1073,  batch time: 0.3478s,  batch average loss: 1.0693\n",
      "Training  ------>   Batch count: 580/1073,  batch time: 0.3760s,  batch average loss: 1.0692\n",
      "Training  ------>   Batch count: 581/1073,  batch time: 0.3750s,  batch average loss: 1.0692\n",
      "Training  ------>   Batch count: 582/1073,  batch time: 0.3750s,  batch average loss: 1.0691\n",
      "Training  ------>   Batch count: 583/1073,  batch time: 0.3407s,  batch average loss: 1.0690\n",
      "Training  ------>   Batch count: 584/1073,  batch time: 0.3125s,  batch average loss: 1.0689\n",
      "Training  ------>   Batch count: 585/1073,  batch time: 0.2778s,  batch average loss: 1.0688\n",
      "Training  ------>   Batch count: 586/1073,  batch time: 0.2986s,  batch average loss: 1.0687\n",
      "Training  ------>   Batch count: 587/1073,  batch time: 0.2902s,  batch average loss: 1.0687\n",
      "Training  ------>   Batch count: 588/1073,  batch time: 0.3407s,  batch average loss: 1.0686\n",
      "Training  ------>   Batch count: 589/1073,  batch time: 0.3654s,  batch average loss: 1.0685\n",
      "Training  ------>   Batch count: 590/1073,  batch time: 0.3518s,  batch average loss: 1.0684\n",
      "Training  ------>   Batch count: 591/1073,  batch time: 0.3871s,  batch average loss: 1.0683\n",
      "Training  ------>   Batch count: 592/1073,  batch time: 0.3750s,  batch average loss: 1.0681\n",
      "Training  ------>   Batch count: 593/1073,  batch time: 0.3125s,  batch average loss: 1.0680\n",
      "Training  ------>   Batch count: 594/1073,  batch time: 0.3125s,  batch average loss: 1.0679\n",
      "Training  ------>   Batch count: 595/1073,  batch time: 0.2980s,  batch average loss: 1.0679\n",
      "Training  ------>   Batch count: 596/1073,  batch time: 0.2969s,  batch average loss: 1.0678\n",
      "Training  ------>   Batch count: 597/1073,  batch time: 0.3081s,  batch average loss: 1.0676\n",
      "Training  ------>   Batch count: 598/1073,  batch time: 0.2969s,  batch average loss: 1.0675\n",
      "Training  ------>   Batch count: 599/1073,  batch time: 0.3437s,  batch average loss: 1.0675\n",
      "Training  ------>   Batch count: 600/1073,  batch time: 0.3281s,  batch average loss: 1.0675\n",
      "Training  ------>   Batch count: 601/1073,  batch time: 0.2969s,  batch average loss: 1.0674\n",
      "Training  ------>   Batch count: 602/1073,  batch time: 0.3437s,  batch average loss: 1.0673\n",
      "Training  ------>   Batch count: 603/1073,  batch time: 0.3715s,  batch average loss: 1.0672\n",
      "Training  ------>   Batch count: 604/1073,  batch time: 0.3594s,  batch average loss: 1.0672\n",
      "Training  ------>   Batch count: 605/1073,  batch time: 0.3125s,  batch average loss: 1.0671\n",
      "Training  ------>   Batch count: 606/1073,  batch time: 0.2969s,  batch average loss: 1.0670\n",
      "Training  ------>   Batch count: 607/1073,  batch time: 0.3125s,  batch average loss: 1.0669\n",
      "Training  ------>   Batch count: 608/1073,  batch time: 0.2969s,  batch average loss: 1.0669\n",
      "Training  ------>   Batch count: 609/1073,  batch time: 0.3090s,  batch average loss: 1.0668\n",
      "Training  ------>   Batch count: 610/1073,  batch time: 0.2812s,  batch average loss: 1.0668\n",
      "Training  ------>   Batch count: 611/1073,  batch time: 0.2969s,  batch average loss: 1.0667\n",
      "Training  ------>   Batch count: 612/1073,  batch time: 0.2969s,  batch average loss: 1.0666\n",
      "Training  ------>   Batch count: 613/1073,  batch time: 0.3437s,  batch average loss: 1.0666\n",
      "Training  ------>   Batch count: 614/1073,  batch time: 0.3281s,  batch average loss: 1.0666\n",
      "Training  ------>   Batch count: 615/1073,  batch time: 0.3237s,  batch average loss: 1.0665\n",
      "Training  ------>   Batch count: 616/1073,  batch time: 0.4218s,  batch average loss: 1.0664\n",
      "Training  ------>   Batch count: 617/1073,  batch time: 0.3125s,  batch average loss: 1.0663\n",
      "Training  ------>   Batch count: 618/1073,  batch time: 0.4375s,  batch average loss: 1.0662\n",
      "Training  ------>   Batch count: 619/1073,  batch time: 0.2812s,  batch average loss: 1.0661\n",
      "Training  ------>   Batch count: 620/1073,  batch time: 0.3594s,  batch average loss: 1.0661\n",
      "Training  ------>   Batch count: 621/1073,  batch time: 0.3560s,  batch average loss: 1.0660\n",
      "Training  ------>   Batch count: 622/1073,  batch time: 0.3750s,  batch average loss: 1.0659\n",
      "Training  ------>   Batch count: 623/1073,  batch time: 0.3281s,  batch average loss: 1.0659\n",
      "Training  ------>   Batch count: 624/1073,  batch time: 0.3125s,  batch average loss: 1.0658\n",
      "Training  ------>   Batch count: 625/1073,  batch time: 0.3427s,  batch average loss: 1.0657\n",
      "Training  ------>   Batch count: 626/1073,  batch time: 0.3246s,  batch average loss: 1.0657\n",
      "Training  ------>   Batch count: 627/1073,  batch time: 0.3192s,  batch average loss: 1.0656\n",
      "Training  ------>   Batch count: 628/1073,  batch time: 0.4218s,  batch average loss: 1.0655\n",
      "Training  ------>   Batch count: 629/1073,  batch time: 0.4311s,  batch average loss: 1.0654\n",
      "Training  ------>   Batch count: 630/1073,  batch time: 0.3125s,  batch average loss: 1.0654\n",
      "Training  ------>   Batch count: 631/1073,  batch time: 0.3906s,  batch average loss: 1.0652\n",
      "Training  ------>   Batch count: 632/1073,  batch time: 0.4340s,  batch average loss: 1.0652\n",
      "Training  ------>   Batch count: 633/1073,  batch time: 0.2969s,  batch average loss: 1.0651\n",
      "Training  ------>   Batch count: 634/1073,  batch time: 0.2812s,  batch average loss: 1.0650\n",
      "Training  ------>   Batch count: 635/1073,  batch time: 0.3088s,  batch average loss: 1.0650\n",
      "Training  ------>   Batch count: 636/1073,  batch time: 0.2656s,  batch average loss: 1.0649\n",
      "Training  ------>   Batch count: 637/1073,  batch time: 0.2968s,  batch average loss: 1.0648\n",
      "Training  ------>   Batch count: 638/1073,  batch time: 0.4183s,  batch average loss: 1.0648\n",
      "Training  ------>   Batch count: 639/1073,  batch time: 0.3437s,  batch average loss: 1.0647\n",
      "Training  ------>   Batch count: 640/1073,  batch time: 0.3281s,  batch average loss: 1.0646\n",
      "Training  ------>   Batch count: 641/1073,  batch time: 0.3437s,  batch average loss: 1.0646\n",
      "Training  ------>   Batch count: 642/1073,  batch time: 0.3125s,  batch average loss: 1.0645\n",
      "Training  ------>   Batch count: 643/1073,  batch time: 0.3080s,  batch average loss: 1.0644\n",
      "Training  ------>   Batch count: 644/1073,  batch time: 0.3128s,  batch average loss: 1.0643\n",
      "Training  ------>   Batch count: 645/1073,  batch time: 0.2969s,  batch average loss: 1.0642\n",
      "Training  ------>   Batch count: 646/1073,  batch time: 0.3281s,  batch average loss: 1.0641\n",
      "Training  ------>   Batch count: 647/1073,  batch time: 0.3125s,  batch average loss: 1.0640\n",
      "Training  ------>   Batch count: 648/1073,  batch time: 0.2812s,  batch average loss: 1.0639\n",
      "Training  ------>   Batch count: 649/1073,  batch time: 0.3125s,  batch average loss: 1.0638\n",
      "Training  ------>   Batch count: 650/1073,  batch time: 0.3288s,  batch average loss: 1.0638\n",
      "Training  ------>   Batch count: 651/1073,  batch time: 0.3459s,  batch average loss: 1.0637\n",
      "Training  ------>   Batch count: 652/1073,  batch time: 0.3437s,  batch average loss: 1.0636\n",
      "Training  ------>   Batch count: 653/1073,  batch time: 0.2812s,  batch average loss: 1.0635\n",
      "Training  ------>   Batch count: 654/1073,  batch time: 0.4062s,  batch average loss: 1.0634\n",
      "Training  ------>   Batch count: 655/1073,  batch time: 0.3090s,  batch average loss: 1.0634\n",
      "Training  ------>   Batch count: 656/1073,  batch time: 0.2969s,  batch average loss: 1.0633\n",
      "Training  ------>   Batch count: 657/1073,  batch time: 0.3125s,  batch average loss: 1.0632\n",
      "Training  ------>   Batch count: 658/1073,  batch time: 0.3125s,  batch average loss: 1.0631\n",
      "Training  ------>   Batch count: 659/1073,  batch time: 0.3906s,  batch average loss: 1.0630\n",
      "Training  ------>   Batch count: 660/1073,  batch time: 0.3281s,  batch average loss: 1.0629\n",
      "Training  ------>   Batch count: 661/1073,  batch time: 0.3558s,  batch average loss: 1.0628\n",
      "Training  ------>   Batch count: 662/1073,  batch time: 0.2812s,  batch average loss: 1.0627\n",
      "Training  ------>   Batch count: 663/1073,  batch time: 0.4062s,  batch average loss: 1.0626\n",
      "Training  ------>   Batch count: 664/1073,  batch time: 0.2969s,  batch average loss: 1.0625\n",
      "Training  ------>   Batch count: 665/1073,  batch time: 0.3125s,  batch average loss: 1.0625\n",
      "Training  ------>   Batch count: 666/1073,  batch time: 0.3125s,  batch average loss: 1.0624\n",
      "Training  ------>   Batch count: 667/1073,  batch time: 0.3246s,  batch average loss: 1.0623\n",
      "Training  ------>   Batch count: 668/1073,  batch time: 0.2968s,  batch average loss: 1.0622\n",
      "Training  ------>   Batch count: 669/1073,  batch time: 0.2812s,  batch average loss: 1.0622\n",
      "Training  ------>   Batch count: 670/1073,  batch time: 0.3125s,  batch average loss: 1.0621\n",
      "Training  ------>   Batch count: 671/1073,  batch time: 0.3437s,  batch average loss: 1.0620\n",
      "Training  ------>   Batch count: 672/1073,  batch time: 0.3437s,  batch average loss: 1.0619\n",
      "Training  ------>   Batch count: 673/1073,  batch time: 0.2924s,  batch average loss: 1.0619\n",
      "Training  ------>   Batch count: 674/1073,  batch time: 0.3437s,  batch average loss: 1.0618\n",
      "Training  ------>   Batch count: 675/1073,  batch time: 0.2969s,  batch average loss: 1.0617\n",
      "Training  ------>   Batch count: 676/1073,  batch time: 0.2968s,  batch average loss: 1.0616\n",
      "Training  ------>   Batch count: 677/1073,  batch time: 0.3437s,  batch average loss: 1.0616\n",
      "Training  ------>   Batch count: 678/1073,  batch time: 0.3750s,  batch average loss: 1.0615\n",
      "Training  ------>   Batch count: 679/1073,  batch time: 0.3869s,  batch average loss: 1.0614\n",
      "Training  ------>   Batch count: 680/1073,  batch time: 0.3906s,  batch average loss: 1.0614\n",
      "Training  ------>   Batch count: 681/1073,  batch time: 0.2969s,  batch average loss: 1.0613\n",
      "Training  ------>   Batch count: 682/1073,  batch time: 0.3125s,  batch average loss: 1.0612\n",
      "Training  ------>   Batch count: 683/1073,  batch time: 0.3281s,  batch average loss: 1.0611\n",
      "Training  ------>   Batch count: 684/1073,  batch time: 0.2969s,  batch average loss: 1.0610\n",
      "Training  ------>   Batch count: 685/1073,  batch time: 0.3168s,  batch average loss: 1.0610\n",
      "Training  ------>   Batch count: 686/1073,  batch time: 0.2969s,  batch average loss: 1.0609\n",
      "Training  ------>   Batch count: 687/1073,  batch time: 0.3125s,  batch average loss: 1.0608\n",
      "Training  ------>   Batch count: 688/1073,  batch time: 0.3449s,  batch average loss: 1.0607\n",
      "Training  ------>   Batch count: 689/1073,  batch time: 0.3281s,  batch average loss: 1.0606\n",
      "Training  ------>   Batch count: 690/1073,  batch time: 0.3833s,  batch average loss: 1.0605\n",
      "Training  ------>   Batch count: 691/1073,  batch time: 0.3247s,  batch average loss: 1.0605\n",
      "Training  ------>   Batch count: 692/1073,  batch time: 0.3848s,  batch average loss: 1.0604\n",
      "Training  ------>   Batch count: 693/1073,  batch time: 0.3906s,  batch average loss: 1.0603\n",
      "Training  ------>   Batch count: 694/1073,  batch time: 0.3750s,  batch average loss: 1.0603\n",
      "Training  ------>   Batch count: 695/1073,  batch time: 0.3437s,  batch average loss: 1.0603\n",
      "Training  ------>   Batch count: 696/1073,  batch time: 0.3393s,  batch average loss: 1.0602\n",
      "Training  ------>   Batch count: 697/1073,  batch time: 0.2969s,  batch average loss: 1.0601\n",
      "Training  ------>   Batch count: 698/1073,  batch time: 0.2812s,  batch average loss: 1.0600\n",
      "Training  ------>   Batch count: 699/1073,  batch time: 0.3750s,  batch average loss: 1.0599\n",
      "Training  ------>   Batch count: 700/1073,  batch time: 0.3281s,  batch average loss: 1.0598\n",
      "Training  ------>   Batch count: 701/1073,  batch time: 0.2986s,  batch average loss: 1.0597\n",
      "Training  ------>   Batch count: 702/1073,  batch time: 0.3281s,  batch average loss: 1.0596\n",
      "Training  ------>   Batch count: 703/1073,  batch time: 0.2952s,  batch average loss: 1.0596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 704/1073,  batch time: 0.2969s,  batch average loss: 1.0594\n",
      "Training  ------>   Batch count: 705/1073,  batch time: 0.3281s,  batch average loss: 1.0594\n",
      "Training  ------>   Batch count: 706/1073,  batch time: 0.3437s,  batch average loss: 1.0593\n",
      "Training  ------>   Batch count: 707/1073,  batch time: 0.3437s,  batch average loss: 1.0593\n",
      "Training  ------>   Batch count: 708/1073,  batch time: 0.3782s,  batch average loss: 1.0592\n",
      "Training  ------>   Batch count: 709/1073,  batch time: 0.3035s,  batch average loss: 1.0591\n",
      "Training  ------>   Batch count: 710/1073,  batch time: 0.2812s,  batch average loss: 1.0590\n",
      "Training  ------>   Batch count: 711/1073,  batch time: 0.3125s,  batch average loss: 1.0589\n",
      "Training  ------>   Batch count: 712/1073,  batch time: 0.2812s,  batch average loss: 1.0589\n",
      "Training  ------>   Batch count: 713/1073,  batch time: 0.3437s,  batch average loss: 1.0588\n",
      "Training  ------>   Batch count: 714/1073,  batch time: 0.3750s,  batch average loss: 1.0587\n",
      "Training  ------>   Batch count: 715/1073,  batch time: 0.3361s,  batch average loss: 1.0586\n",
      "Training  ------>   Batch count: 716/1073,  batch time: 0.3437s,  batch average loss: 1.0586\n",
      "Training  ------>   Batch count: 717/1073,  batch time: 0.3750s,  batch average loss: 1.0585\n",
      "Training  ------>   Batch count: 718/1073,  batch time: 0.3281s,  batch average loss: 1.0584\n",
      "Training  ------>   Batch count: 719/1073,  batch time: 0.3437s,  batch average loss: 1.0584\n",
      "Training  ------>   Batch count: 720/1073,  batch time: 0.3080s,  batch average loss: 1.0583\n",
      "Training  ------>   Batch count: 721/1073,  batch time: 0.2852s,  batch average loss: 1.0582\n",
      "Training  ------>   Batch count: 722/1073,  batch time: 0.3594s,  batch average loss: 1.0582\n",
      "Training  ------>   Batch count: 723/1073,  batch time: 0.3625s,  batch average loss: 1.0581\n",
      "Training  ------>   Batch count: 724/1073,  batch time: 0.2812s,  batch average loss: 1.0581\n",
      "Training  ------>   Batch count: 725/1073,  batch time: 0.2812s,  batch average loss: 1.0580\n",
      "Training  ------>   Batch count: 726/1073,  batch time: 0.3080s,  batch average loss: 1.0579\n",
      "Training  ------>   Batch count: 727/1073,  batch time: 0.2812s,  batch average loss: 1.0578\n",
      "Training  ------>   Batch count: 728/1073,  batch time: 0.3281s,  batch average loss: 1.0577\n",
      "Training  ------>   Batch count: 729/1073,  batch time: 0.3125s,  batch average loss: 1.0576\n",
      "Training  ------>   Batch count: 730/1073,  batch time: 0.3594s,  batch average loss: 1.0576\n",
      "Training  ------>   Batch count: 731/1073,  batch time: 0.2969s,  batch average loss: 1.0575\n",
      "Training  ------>   Batch count: 732/1073,  batch time: 0.3121s,  batch average loss: 1.0574\n",
      "Training  ------>   Batch count: 733/1073,  batch time: 0.2944s,  batch average loss: 1.0573\n",
      "Training  ------>   Batch count: 734/1073,  batch time: 0.2969s,  batch average loss: 1.0572\n",
      "Training  ------>   Batch count: 735/1073,  batch time: 0.3419s,  batch average loss: 1.0571\n",
      "Training  ------>   Batch count: 736/1073,  batch time: 0.3125s,  batch average loss: 1.0570\n",
      "Training  ------>   Batch count: 737/1073,  batch time: 0.3125s,  batch average loss: 1.0569\n",
      "Training  ------>   Batch count: 738/1073,  batch time: 0.3906s,  batch average loss: 1.0568\n",
      "Training  ------>   Batch count: 739/1073,  batch time: 0.2747s,  batch average loss: 1.0567\n",
      "Training  ------>   Batch count: 740/1073,  batch time: 0.3437s,  batch average loss: 1.0567\n",
      "Training  ------>   Batch count: 741/1073,  batch time: 0.3125s,  batch average loss: 1.0566\n",
      "Training  ------>   Batch count: 742/1073,  batch time: 0.3125s,  batch average loss: 1.0566\n",
      "Training  ------>   Batch count: 743/1073,  batch time: 0.2969s,  batch average loss: 1.0565\n",
      "Training  ------>   Batch count: 744/1073,  batch time: 0.3750s,  batch average loss: 1.0564\n",
      "Training  ------>   Batch count: 745/1073,  batch time: 0.3237s,  batch average loss: 1.0563\n",
      "Training  ------>   Batch count: 746/1073,  batch time: 0.3750s,  batch average loss: 1.0563\n",
      "Training  ------>   Batch count: 747/1073,  batch time: 0.3593s,  batch average loss: 1.0562\n",
      "Training  ------>   Batch count: 748/1073,  batch time: 0.3281s,  batch average loss: 1.0561\n",
      "Training  ------>   Batch count: 749/1073,  batch time: 0.2969s,  batch average loss: 1.0560\n",
      "Training  ------>   Batch count: 750/1073,  batch time: 0.2812s,  batch average loss: 1.0559\n",
      "Training  ------>   Batch count: 751/1073,  batch time: 0.3237s,  batch average loss: 1.0558\n",
      "Training  ------>   Batch count: 752/1073,  batch time: 0.3125s,  batch average loss: 1.0557\n",
      "Training  ------>   Batch count: 753/1073,  batch time: 0.3281s,  batch average loss: 1.0556\n",
      "Training  ------>   Batch count: 754/1073,  batch time: 0.3125s,  batch average loss: 1.0554\n",
      "Training  ------>   Batch count: 755/1073,  batch time: 0.3125s,  batch average loss: 1.0553\n",
      "Training  ------>   Batch count: 756/1073,  batch time: 0.2968s,  batch average loss: 1.0553\n",
      "Training  ------>   Batch count: 757/1073,  batch time: 0.2926s,  batch average loss: 1.0552\n",
      "Training  ------>   Batch count: 758/1073,  batch time: 0.3437s,  batch average loss: 1.0552\n",
      "Training  ------>   Batch count: 759/1073,  batch time: 0.3437s,  batch average loss: 1.0552\n",
      "Training  ------>   Batch count: 760/1073,  batch time: 0.2969s,  batch average loss: 1.0551\n",
      "Training  ------>   Batch count: 761/1073,  batch time: 0.4062s,  batch average loss: 1.0550\n",
      "Training  ------>   Batch count: 762/1073,  batch time: 0.3593s,  batch average loss: 1.0550\n",
      "Training  ------>   Batch count: 763/1073,  batch time: 0.3550s,  batch average loss: 1.0549\n",
      "Training  ------>   Batch count: 764/1073,  batch time: 0.3750s,  batch average loss: 1.0548\n",
      "Training  ------>   Batch count: 765/1073,  batch time: 0.3426s,  batch average loss: 1.0547\n",
      "Training  ------>   Batch count: 766/1073,  batch time: 0.3437s,  batch average loss: 1.0546\n",
      "Training  ------>   Batch count: 767/1073,  batch time: 0.3281s,  batch average loss: 1.0545\n",
      "Training  ------>   Batch count: 768/1073,  batch time: 0.3237s,  batch average loss: 1.0544\n",
      "Training  ------>   Batch count: 769/1073,  batch time: 0.2969s,  batch average loss: 1.0543\n",
      "Training  ------>   Batch count: 770/1073,  batch time: 0.3750s,  batch average loss: 1.0542\n",
      "Training  ------>   Batch count: 771/1073,  batch time: 0.3281s,  batch average loss: 1.0542\n",
      "Training  ------>   Batch count: 772/1073,  batch time: 0.3136s,  batch average loss: 1.0541\n",
      "Training  ------>   Batch count: 773/1073,  batch time: 0.3750s,  batch average loss: 1.0540\n",
      "Training  ------>   Batch count: 774/1073,  batch time: 0.3080s,  batch average loss: 1.0539\n",
      "Training  ------>   Batch count: 775/1073,  batch time: 0.2969s,  batch average loss: 1.0538\n",
      "Training  ------>   Batch count: 776/1073,  batch time: 0.3281s,  batch average loss: 1.0537\n",
      "Training  ------>   Batch count: 777/1073,  batch time: 0.3125s,  batch average loss: 1.0536\n",
      "Training  ------>   Batch count: 778/1073,  batch time: 0.3125s,  batch average loss: 1.0535\n",
      "Training  ------>   Batch count: 779/1073,  batch time: 0.3437s,  batch average loss: 1.0535\n",
      "Training  ------>   Batch count: 780/1073,  batch time: 0.3080s,  batch average loss: 1.0534\n",
      "Training  ------>   Batch count: 781/1073,  batch time: 0.3125s,  batch average loss: 1.0533\n",
      "Training  ------>   Batch count: 782/1073,  batch time: 0.3125s,  batch average loss: 1.0533\n",
      "Training  ------>   Batch count: 783/1073,  batch time: 0.2969s,  batch average loss: 1.0532\n",
      "Training  ------>   Batch count: 784/1073,  batch time: 0.4218s,  batch average loss: 1.0531\n",
      "Training  ------>   Batch count: 785/1073,  batch time: 0.3750s,  batch average loss: 1.0530\n",
      "Training  ------>   Batch count: 786/1073,  batch time: 0.3393s,  batch average loss: 1.0529\n",
      "Training  ------>   Batch count: 787/1073,  batch time: 0.4062s,  batch average loss: 1.0528\n",
      "Training  ------>   Batch count: 788/1073,  batch time: 0.3125s,  batch average loss: 1.0528\n",
      "Training  ------>   Batch count: 789/1073,  batch time: 0.3437s,  batch average loss: 1.0527\n",
      "Training  ------>   Batch count: 790/1073,  batch time: 0.3125s,  batch average loss: 1.0526\n",
      "Training  ------>   Batch count: 791/1073,  batch time: 0.3437s,  batch average loss: 1.0526\n",
      "Training  ------>   Batch count: 792/1073,  batch time: 0.3237s,  batch average loss: 1.0525\n",
      "Training  ------>   Batch count: 793/1073,  batch time: 0.2969s,  batch average loss: 1.0525\n",
      "Training  ------>   Batch count: 794/1073,  batch time: 0.3125s,  batch average loss: 1.0524\n",
      "Training  ------>   Batch count: 795/1073,  batch time: 0.3090s,  batch average loss: 1.0523\n",
      "Training  ------>   Batch count: 796/1073,  batch time: 0.3437s,  batch average loss: 1.0522\n",
      "Training  ------>   Batch count: 797/1073,  batch time: 0.3861s,  batch average loss: 1.0521\n",
      "Training  ------>   Batch count: 798/1073,  batch time: 0.2969s,  batch average loss: 1.0520\n",
      "Training  ------>   Batch count: 799/1073,  batch time: 0.2969s,  batch average loss: 1.0519\n",
      "Training  ------>   Batch count: 800/1073,  batch time: 0.3125s,  batch average loss: 1.0519\n",
      "Training  ------>   Batch count: 801/1073,  batch time: 0.3281s,  batch average loss: 1.0518\n",
      "Training  ------>   Batch count: 802/1073,  batch time: 0.3594s,  batch average loss: 1.0518\n",
      "Training  ------>   Batch count: 803/1073,  batch time: 0.3468s,  batch average loss: 1.0517\n",
      "Training  ------>   Batch count: 804/1073,  batch time: 0.2979s,  batch average loss: 1.0516\n",
      "Training  ------>   Batch count: 805/1073,  batch time: 0.3281s,  batch average loss: 1.0515\n",
      "Training  ------>   Batch count: 806/1073,  batch time: 0.3281s,  batch average loss: 1.0515\n",
      "Training  ------>   Batch count: 807/1073,  batch time: 0.3125s,  batch average loss: 1.0514\n",
      "Training  ------>   Batch count: 808/1073,  batch time: 0.3281s,  batch average loss: 1.0513\n",
      "Training  ------>   Batch count: 809/1073,  batch time: 0.3281s,  batch average loss: 1.0512\n",
      "Training  ------>   Batch count: 810/1073,  batch time: 0.3237s,  batch average loss: 1.0511\n",
      "Training  ------>   Batch count: 811/1073,  batch time: 0.3594s,  batch average loss: 1.0510\n",
      "Training  ------>   Batch count: 812/1073,  batch time: 0.3186s,  batch average loss: 1.0510\n",
      "Training  ------>   Batch count: 813/1073,  batch time: 0.2839s,  batch average loss: 1.0509\n",
      "Training  ------>   Batch count: 814/1073,  batch time: 0.3437s,  batch average loss: 1.0508\n",
      "Training  ------>   Batch count: 815/1073,  batch time: 0.3125s,  batch average loss: 1.0508\n",
      "Training  ------>   Batch count: 816/1073,  batch time: 0.3237s,  batch average loss: 1.0507\n",
      "Training  ------>   Batch count: 817/1073,  batch time: 0.3281s,  batch average loss: 1.0506\n",
      "Training  ------>   Batch count: 818/1073,  batch time: 0.3906s,  batch average loss: 1.0505\n",
      "Training  ------>   Batch count: 819/1073,  batch time: 0.2969s,  batch average loss: 1.0505\n",
      "Training  ------>   Batch count: 820/1073,  batch time: 0.3125s,  batch average loss: 1.0504\n",
      "Training  ------>   Batch count: 821/1073,  batch time: 0.3281s,  batch average loss: 1.0504\n",
      "Training  ------>   Batch count: 822/1073,  batch time: 0.3089s,  batch average loss: 1.0503\n",
      "Training  ------>   Batch count: 823/1073,  batch time: 0.3281s,  batch average loss: 1.0502\n",
      "Training  ------>   Batch count: 824/1073,  batch time: 0.3906s,  batch average loss: 1.0502\n",
      "Training  ------>   Batch count: 825/1073,  batch time: 0.3281s,  batch average loss: 1.0501\n",
      "Training  ------>   Batch count: 826/1073,  batch time: 0.2968s,  batch average loss: 1.0500\n",
      "Training  ------>   Batch count: 827/1073,  batch time: 0.3392s,  batch average loss: 1.0499\n",
      "Training  ------>   Batch count: 828/1073,  batch time: 0.3281s,  batch average loss: 1.0498\n",
      "Training  ------>   Batch count: 829/1073,  batch time: 0.3906s,  batch average loss: 1.0498\n",
      "Training  ------>   Batch count: 830/1073,  batch time: 0.2969s,  batch average loss: 1.0498\n",
      "Training  ------>   Batch count: 831/1073,  batch time: 0.2812s,  batch average loss: 1.0497\n",
      "Training  ------>   Batch count: 832/1073,  batch time: 0.3437s,  batch average loss: 1.0496\n",
      "Training  ------>   Batch count: 833/1073,  batch time: 0.3080s,  batch average loss: 1.0495\n",
      "Training  ------>   Batch count: 834/1073,  batch time: 0.3125s,  batch average loss: 1.0494\n",
      "Training  ------>   Batch count: 835/1073,  batch time: 0.3281s,  batch average loss: 1.0493\n",
      "Training  ------>   Batch count: 836/1073,  batch time: 0.3593s,  batch average loss: 1.0493\n",
      "Training  ------>   Batch count: 837/1073,  batch time: 0.3125s,  batch average loss: 1.0492\n",
      "Training  ------>   Batch count: 838/1073,  batch time: 0.3593s,  batch average loss: 1.0491\n",
      "Training  ------>   Batch count: 839/1073,  batch time: 0.3558s,  batch average loss: 1.0491\n",
      "Training  ------>   Batch count: 840/1073,  batch time: 0.3750s,  batch average loss: 1.0490\n",
      "Training  ------>   Batch count: 841/1073,  batch time: 0.3437s,  batch average loss: 1.0490\n",
      "Training  ------>   Batch count: 842/1073,  batch time: 0.2969s,  batch average loss: 1.0489\n",
      "Training  ------>   Batch count: 843/1073,  batch time: 0.3594s,  batch average loss: 1.0488\n",
      "Training  ------>   Batch count: 844/1073,  batch time: 0.3437s,  batch average loss: 1.0487\n",
      "Training  ------>   Batch count: 845/1073,  batch time: 0.3237s,  batch average loss: 1.0486\n",
      "Training  ------>   Batch count: 846/1073,  batch time: 0.2969s,  batch average loss: 1.0486\n",
      "Training  ------>   Batch count: 847/1073,  batch time: 0.2969s,  batch average loss: 1.0485\n",
      "Training  ------>   Batch count: 848/1073,  batch time: 0.3594s,  batch average loss: 1.0485\n",
      "Training  ------>   Batch count: 849/1073,  batch time: 0.3437s,  batch average loss: 1.0484\n",
      "Training  ------>   Batch count: 850/1073,  batch time: 0.3906s,  batch average loss: 1.0483\n",
      "Training  ------>   Batch count: 851/1073,  batch time: 0.4028s,  batch average loss: 1.0482\n",
      "Training  ------>   Batch count: 852/1073,  batch time: 0.3125s,  batch average loss: 1.0481\n",
      "Training  ------>   Batch count: 853/1073,  batch time: 0.3449s,  batch average loss: 1.0480\n",
      "Training  ------>   Batch count: 854/1073,  batch time: 0.3594s,  batch average loss: 1.0479\n",
      "Training  ------>   Batch count: 855/1073,  batch time: 0.3125s,  batch average loss: 1.0479\n",
      "Training  ------>   Batch count: 856/1073,  batch time: 0.4018s,  batch average loss: 1.0478\n",
      "Training  ------>   Batch count: 857/1073,  batch time: 0.2661s,  batch average loss: 1.0477\n",
      "Training  ------>   Batch count: 858/1073,  batch time: 0.3593s,  batch average loss: 1.0476\n",
      "Training  ------>   Batch count: 859/1073,  batch time: 0.3125s,  batch average loss: 1.0476\n",
      "Training  ------>   Batch count: 860/1073,  batch time: 0.3125s,  batch average loss: 1.0475\n",
      "Training  ------>   Batch count: 861/1073,  batch time: 0.2812s,  batch average loss: 1.0474\n",
      "Training  ------>   Batch count: 862/1073,  batch time: 0.2778s,  batch average loss: 1.0474\n",
      "Training  ------>   Batch count: 863/1073,  batch time: 0.3281s,  batch average loss: 1.0473\n",
      "Training  ------>   Batch count: 864/1073,  batch time: 0.4375s,  batch average loss: 1.0472\n",
      "Training  ------>   Batch count: 865/1073,  batch time: 0.3437s,  batch average loss: 1.0471\n",
      "Training  ------>   Batch count: 866/1073,  batch time: 0.3125s,  batch average loss: 1.0471\n",
      "Training  ------>   Batch count: 867/1073,  batch time: 0.3125s,  batch average loss: 1.0469\n",
      "Training  ------>   Batch count: 868/1073,  batch time: 0.2994s,  batch average loss: 1.0469\n",
      "Training  ------>   Batch count: 869/1073,  batch time: 0.2968s,  batch average loss: 1.0468\n",
      "Training  ------>   Batch count: 870/1073,  batch time: 0.3169s,  batch average loss: 1.0467\n",
      "Training  ------>   Batch count: 871/1073,  batch time: 0.3906s,  batch average loss: 1.0467\n",
      "Training  ------>   Batch count: 872/1073,  batch time: 0.3281s,  batch average loss: 1.0466\n",
      "Training  ------>   Batch count: 873/1073,  batch time: 0.3593s,  batch average loss: 1.0465\n",
      "Training  ------>   Batch count: 874/1073,  batch time: 0.3403s,  batch average loss: 1.0464\n",
      "Training  ------>   Batch count: 875/1073,  batch time: 0.2812s,  batch average loss: 1.0463\n",
      "Training  ------>   Batch count: 876/1073,  batch time: 0.3125s,  batch average loss: 1.0463\n",
      "Training  ------>   Batch count: 877/1073,  batch time: 0.2812s,  batch average loss: 1.0462\n",
      "Training  ------>   Batch count: 878/1073,  batch time: 0.3605s,  batch average loss: 1.0461\n",
      "Training  ------>   Batch count: 879/1073,  batch time: 0.3281s,  batch average loss: 1.0460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 880/1073,  batch time: 0.4076s,  batch average loss: 1.0459\n",
      "Training  ------>   Batch count: 881/1073,  batch time: 0.3437s,  batch average loss: 1.0459\n",
      "Training  ------>   Batch count: 882/1073,  batch time: 0.2969s,  batch average loss: 1.0458\n",
      "Training  ------>   Batch count: 883/1073,  batch time: 0.2824s,  batch average loss: 1.0458\n",
      "Training  ------>   Batch count: 884/1073,  batch time: 0.3281s,  batch average loss: 1.0457\n",
      "Training  ------>   Batch count: 885/1073,  batch time: 0.3593s,  batch average loss: 1.0456\n",
      "Training  ------>   Batch count: 886/1073,  batch time: 0.3549s,  batch average loss: 1.0456\n",
      "Training  ------>   Batch count: 887/1073,  batch time: 0.3281s,  batch average loss: 1.0455\n",
      "Training  ------>   Batch count: 888/1073,  batch time: 0.2969s,  batch average loss: 1.0454\n",
      "Training  ------>   Batch count: 889/1073,  batch time: 0.2969s,  batch average loss: 1.0453\n",
      "Training  ------>   Batch count: 890/1073,  batch time: 0.3437s,  batch average loss: 1.0453\n",
      "Training  ------>   Batch count: 891/1073,  batch time: 0.4062s,  batch average loss: 1.0452\n",
      "Training  ------>   Batch count: 892/1073,  batch time: 0.4184s,  batch average loss: 1.0451\n",
      "Training  ------>   Batch count: 893/1073,  batch time: 0.3125s,  batch average loss: 1.0450\n",
      "Training  ------>   Batch count: 894/1073,  batch time: 0.2969s,  batch average loss: 1.0449\n",
      "Training  ------>   Batch count: 895/1073,  batch time: 0.3398s,  batch average loss: 1.0448\n",
      "Training  ------>   Batch count: 896/1073,  batch time: 0.3594s,  batch average loss: 1.0448\n",
      "Training  ------>   Batch count: 897/1073,  batch time: 0.3392s,  batch average loss: 1.0447\n",
      "Training  ------>   Batch count: 898/1073,  batch time: 0.3125s,  batch average loss: 1.0446\n",
      "Training  ------>   Batch count: 899/1073,  batch time: 0.3750s,  batch average loss: 1.0445\n",
      "Training  ------>   Batch count: 900/1073,  batch time: 0.3162s,  batch average loss: 1.0445\n",
      "Training  ------>   Batch count: 901/1073,  batch time: 0.4375s,  batch average loss: 1.0444\n",
      "Training  ------>   Batch count: 902/1073,  batch time: 0.3281s,  batch average loss: 1.0443\n",
      "Training  ------>   Batch count: 903/1073,  batch time: 0.2778s,  batch average loss: 1.0443\n",
      "Training  ------>   Batch count: 904/1073,  batch time: 0.3125s,  batch average loss: 1.0442\n",
      "Training  ------>   Batch count: 905/1073,  batch time: 0.3437s,  batch average loss: 1.0442\n",
      "Training  ------>   Batch count: 906/1073,  batch time: 0.4110s,  batch average loss: 1.0441\n",
      "Training  ------>   Batch count: 907/1073,  batch time: 0.2656s,  batch average loss: 1.0440\n",
      "Training  ------>   Batch count: 908/1073,  batch time: 0.3125s,  batch average loss: 1.0439\n",
      "Training  ------>   Batch count: 909/1073,  batch time: 0.3246s,  batch average loss: 1.0439\n",
      "Training  ------>   Batch count: 910/1073,  batch time: 0.3281s,  batch average loss: 1.0438\n",
      "Training  ------>   Batch count: 911/1073,  batch time: 0.3281s,  batch average loss: 1.0437\n",
      "Training  ------>   Batch count: 912/1073,  batch time: 0.3593s,  batch average loss: 1.0436\n",
      "Training  ------>   Batch count: 913/1073,  batch time: 0.3281s,  batch average loss: 1.0436\n",
      "Training  ------>   Batch count: 914/1073,  batch time: 0.3281s,  batch average loss: 1.0435\n",
      "Training  ------>   Batch count: 915/1073,  batch time: 0.3080s,  batch average loss: 1.0434\n",
      "Training  ------>   Batch count: 916/1073,  batch time: 0.3281s,  batch average loss: 1.0433\n",
      "Training  ------>   Batch count: 917/1073,  batch time: 0.3750s,  batch average loss: 1.0433\n",
      "Training  ------>   Batch count: 918/1073,  batch time: 0.3294s,  batch average loss: 1.0432\n",
      "Training  ------>   Batch count: 919/1073,  batch time: 0.3125s,  batch average loss: 1.0432\n",
      "Training  ------>   Batch count: 920/1073,  batch time: 0.3258s,  batch average loss: 1.0431\n",
      "Training  ------>   Batch count: 921/1073,  batch time: 0.3480s,  batch average loss: 1.0431\n",
      "Training  ------>   Batch count: 922/1073,  batch time: 0.2969s,  batch average loss: 1.0430\n",
      "Training  ------>   Batch count: 923/1073,  batch time: 0.3568s,  batch average loss: 1.0430\n",
      "Training  ------>   Batch count: 924/1073,  batch time: 0.2968s,  batch average loss: 1.0429\n",
      "Training  ------>   Batch count: 925/1073,  batch time: 0.3125s,  batch average loss: 1.0428\n",
      "Training  ------>   Batch count: 926/1073,  batch time: 0.2656s,  batch average loss: 1.0428\n",
      "Training  ------>   Batch count: 927/1073,  batch time: 0.2925s,  batch average loss: 1.0427\n",
      "Training  ------>   Batch count: 928/1073,  batch time: 0.3750s,  batch average loss: 1.0426\n",
      "Training  ------>   Batch count: 929/1073,  batch time: 0.3281s,  batch average loss: 1.0425\n",
      "Training  ------>   Batch count: 930/1073,  batch time: 0.2812s,  batch average loss: 1.0424\n",
      "Training  ------>   Batch count: 931/1073,  batch time: 0.2969s,  batch average loss: 1.0424\n",
      "Training  ------>   Batch count: 932/1073,  batch time: 0.3281s,  batch average loss: 1.0423\n",
      "Training  ------>   Batch count: 933/1073,  batch time: 0.3392s,  batch average loss: 1.0422\n",
      "Training  ------>   Batch count: 934/1073,  batch time: 0.3437s,  batch average loss: 1.0421\n",
      "Training  ------>   Batch count: 935/1073,  batch time: 0.3281s,  batch average loss: 1.0420\n",
      "Training  ------>   Batch count: 936/1073,  batch time: 0.2969s,  batch average loss: 1.0419\n",
      "Training  ------>   Batch count: 937/1073,  batch time: 0.3437s,  batch average loss: 1.0418\n",
      "Training  ------>   Batch count: 938/1073,  batch time: 0.3437s,  batch average loss: 1.0418\n",
      "Training  ------>   Batch count: 939/1073,  batch time: 0.3236s,  batch average loss: 1.0418\n",
      "Training  ------>   Batch count: 940/1073,  batch time: 0.3281s,  batch average loss: 1.0417\n",
      "Training  ------>   Batch count: 941/1073,  batch time: 0.3281s,  batch average loss: 1.0416\n",
      "Training  ------>   Batch count: 942/1073,  batch time: 0.3125s,  batch average loss: 1.0416\n",
      "Training  ------>   Batch count: 943/1073,  batch time: 0.3906s,  batch average loss: 1.0415\n",
      "Training  ------>   Batch count: 944/1073,  batch time: 0.3332s,  batch average loss: 1.0414\n",
      "Training  ------>   Batch count: 945/1073,  batch time: 0.3437s,  batch average loss: 1.0413\n",
      "Training  ------>   Batch count: 946/1073,  batch time: 0.2969s,  batch average loss: 1.0412\n",
      "Training  ------>   Batch count: 947/1073,  batch time: 0.4074s,  batch average loss: 1.0412\n",
      "Training  ------>   Batch count: 948/1073,  batch time: 0.3281s,  batch average loss: 1.0411\n",
      "Training  ------>   Batch count: 949/1073,  batch time: 0.3750s,  batch average loss: 1.0410\n",
      "Training  ------>   Batch count: 950/1073,  batch time: 0.3413s,  batch average loss: 1.0409\n",
      "Training  ------>   Batch count: 951/1073,  batch time: 0.2812s,  batch average loss: 1.0409\n",
      "Training  ------>   Batch count: 952/1073,  batch time: 0.3437s,  batch average loss: 1.0408\n",
      "Training  ------>   Batch count: 953/1073,  batch time: 0.2812s,  batch average loss: 1.0407\n",
      "Training  ------>   Batch count: 954/1073,  batch time: 0.3281s,  batch average loss: 1.0407\n",
      "Training  ------>   Batch count: 955/1073,  batch time: 0.3125s,  batch average loss: 1.0406\n",
      "Training  ------>   Batch count: 956/1073,  batch time: 0.2924s,  batch average loss: 1.0406\n",
      "Training  ------>   Batch count: 957/1073,  batch time: 0.3125s,  batch average loss: 1.0405\n",
      "Training  ------>   Batch count: 958/1073,  batch time: 0.2969s,  batch average loss: 1.0404\n",
      "Training  ------>   Batch count: 959/1073,  batch time: 0.2969s,  batch average loss: 1.0404\n",
      "Training  ------>   Batch count: 960/1073,  batch time: 0.3281s,  batch average loss: 1.0403\n",
      "Training  ------>   Batch count: 961/1073,  batch time: 0.2812s,  batch average loss: 1.0402\n",
      "Training  ------>   Batch count: 962/1073,  batch time: 0.2812s,  batch average loss: 1.0402\n",
      "Training  ------>   Batch count: 963/1073,  batch time: 0.3090s,  batch average loss: 1.0401\n",
      "Training  ------>   Batch count: 964/1073,  batch time: 0.2969s,  batch average loss: 1.0400\n",
      "Training  ------>   Batch count: 965/1073,  batch time: 0.3594s,  batch average loss: 1.0400\n",
      "Training  ------>   Batch count: 966/1073,  batch time: 0.3125s,  batch average loss: 1.0399\n",
      "Training  ------>   Batch count: 967/1073,  batch time: 0.2812s,  batch average loss: 1.0398\n",
      "Training  ------>   Batch count: 968/1073,  batch time: 0.2969s,  batch average loss: 1.0397\n",
      "Training  ------>   Batch count: 969/1073,  batch time: 0.2924s,  batch average loss: 1.0397\n",
      "Training  ------>   Batch count: 970/1073,  batch time: 0.3281s,  batch average loss: 1.0396\n",
      "Training  ------>   Batch count: 971/1073,  batch time: 0.3437s,  batch average loss: 1.0395\n",
      "Training  ------>   Batch count: 972/1073,  batch time: 0.3281s,  batch average loss: 1.0394\n",
      "Training  ------>   Batch count: 973/1073,  batch time: 0.2969s,  batch average loss: 1.0393\n",
      "Training  ------>   Batch count: 974/1073,  batch time: 0.3281s,  batch average loss: 1.0392\n",
      "Training  ------>   Batch count: 975/1073,  batch time: 0.3246s,  batch average loss: 1.0392\n",
      "Training  ------>   Batch count: 976/1073,  batch time: 0.2969s,  batch average loss: 1.0391\n",
      "Training  ------>   Batch count: 977/1073,  batch time: 0.3125s,  batch average loss: 1.0390\n",
      "Training  ------>   Batch count: 978/1073,  batch time: 0.3594s,  batch average loss: 1.0390\n",
      "Training  ------>   Batch count: 979/1073,  batch time: 0.3281s,  batch average loss: 1.0389\n",
      "Training  ------>   Batch count: 980/1073,  batch time: 0.3594s,  batch average loss: 1.0388\n",
      "Training  ------>   Batch count: 981/1073,  batch time: 0.2612s,  batch average loss: 1.0388\n",
      "Training  ------>   Batch count: 982/1073,  batch time: 0.3125s,  batch average loss: 1.0387\n",
      "Training  ------>   Batch count: 983/1073,  batch time: 0.2969s,  batch average loss: 1.0387\n",
      "Training  ------>   Batch count: 984/1073,  batch time: 0.3125s,  batch average loss: 1.0386\n",
      "Training  ------>   Batch count: 985/1073,  batch time: 0.3437s,  batch average loss: 1.0385\n",
      "Training  ------>   Batch count: 986/1073,  batch time: 0.2812s,  batch average loss: 1.0384\n",
      "Training  ------>   Batch count: 987/1073,  batch time: 0.3125s,  batch average loss: 1.0384\n",
      "Training  ------>   Batch count: 988/1073,  batch time: 0.3559s,  batch average loss: 1.0383\n",
      "Training  ------>   Batch count: 989/1073,  batch time: 0.2969s,  batch average loss: 1.0382\n",
      "Training  ------>   Batch count: 990/1073,  batch time: 0.3594s,  batch average loss: 1.0382\n",
      "Training  ------>   Batch count: 991/1073,  batch time: 0.3125s,  batch average loss: 1.0381\n",
      "Training  ------>   Batch count: 992/1073,  batch time: 0.3125s,  batch average loss: 1.0379\n",
      "Training  ------>   Batch count: 993/1073,  batch time: 0.2969s,  batch average loss: 1.0379\n",
      "Training  ------>   Batch count: 994/1073,  batch time: 0.3236s,  batch average loss: 1.0378\n",
      "Training  ------>   Batch count: 995/1073,  batch time: 0.3437s,  batch average loss: 1.0377\n",
      "Training  ------>   Batch count: 996/1073,  batch time: 0.2969s,  batch average loss: 1.0376\n",
      "Training  ------>   Batch count: 997/1073,  batch time: 0.2656s,  batch average loss: 1.0376\n",
      "Training  ------>   Batch count: 998/1073,  batch time: 0.2969s,  batch average loss: 1.0375\n",
      "Training  ------>   Batch count: 999/1073,  batch time: 0.3281s,  batch average loss: 1.0374\n",
      "Training  ------>   Batch count: 1000/1073,  batch time: 0.3861s,  batch average loss: 1.0373\n",
      "Training  ------>   Batch count: 1001/1073,  batch time: 0.2969s,  batch average loss: 1.0372\n",
      "Training  ------>   Batch count: 1002/1073,  batch time: 0.2968s,  batch average loss: 1.0372\n",
      "Training  ------>   Batch count: 1003/1073,  batch time: 0.3750s,  batch average loss: 1.0371\n",
      "Training  ------>   Batch count: 1004/1073,  batch time: 0.2968s,  batch average loss: 1.0370\n",
      "Training  ------>   Batch count: 1005/1073,  batch time: 0.2969s,  batch average loss: 1.0370\n",
      "Training  ------>   Batch count: 1006/1073,  batch time: 0.2924s,  batch average loss: 1.0369\n",
      "Training  ------>   Batch count: 1007/1073,  batch time: 0.2812s,  batch average loss: 1.0368\n",
      "Training  ------>   Batch count: 1008/1073,  batch time: 0.3125s,  batch average loss: 1.0368\n",
      "Training  ------>   Batch count: 1009/1073,  batch time: 0.3281s,  batch average loss: 1.0367\n",
      "Training  ------>   Batch count: 1010/1073,  batch time: 0.3125s,  batch average loss: 1.0366\n",
      "Training  ------>   Batch count: 1011/1073,  batch time: 0.2969s,  batch average loss: 1.0365\n",
      "Training  ------>   Batch count: 1012/1073,  batch time: 0.2777s,  batch average loss: 1.0364\n",
      "Training  ------>   Batch count: 1013/1073,  batch time: 0.3281s,  batch average loss: 1.0364\n",
      "Training  ------>   Batch count: 1014/1073,  batch time: 0.2812s,  batch average loss: 1.0363\n",
      "Training  ------>   Batch count: 1015/1073,  batch time: 0.2969s,  batch average loss: 1.0362\n",
      "Training  ------>   Batch count: 1016/1073,  batch time: 0.2812s,  batch average loss: 1.0362\n",
      "Training  ------>   Batch count: 1017/1073,  batch time: 0.3281s,  batch average loss: 1.0361\n",
      "Training  ------>   Batch count: 1018/1073,  batch time: 0.2869s,  batch average loss: 1.0360\n",
      "Training  ------>   Batch count: 1019/1073,  batch time: 0.3081s,  batch average loss: 1.0359\n",
      "Training  ------>   Batch count: 1020/1073,  batch time: 0.2969s,  batch average loss: 1.0358\n",
      "Training  ------>   Batch count: 1021/1073,  batch time: 0.2656s,  batch average loss: 1.0358\n",
      "Training  ------>   Batch count: 1022/1073,  batch time: 0.2969s,  batch average loss: 1.0357\n",
      "Training  ------>   Batch count: 1023/1073,  batch time: 0.3125s,  batch average loss: 1.0356\n",
      "Training  ------>   Batch count: 1024/1073,  batch time: 0.2969s,  batch average loss: 1.0355\n",
      "Training  ------>   Batch count: 1025/1073,  batch time: 0.3393s,  batch average loss: 1.0355\n",
      "Training  ------>   Batch count: 1026/1073,  batch time: 0.2812s,  batch average loss: 1.0354\n",
      "Training  ------>   Batch count: 1027/1073,  batch time: 0.3125s,  batch average loss: 1.0353\n",
      "Training  ------>   Batch count: 1028/1073,  batch time: 0.3125s,  batch average loss: 1.0353\n",
      "Training  ------>   Batch count: 1029/1073,  batch time: 0.3750s,  batch average loss: 1.0352\n",
      "Training  ------>   Batch count: 1030/1073,  batch time: 0.3437s,  batch average loss: 1.0352\n",
      "Training  ------>   Batch count: 1031/1073,  batch time: 0.3393s,  batch average loss: 1.0351\n",
      "Training  ------>   Batch count: 1032/1073,  batch time: 0.3125s,  batch average loss: 1.0350\n",
      "Training  ------>   Batch count: 1033/1073,  batch time: 0.3292s,  batch average loss: 1.0349\n",
      "Training  ------>   Batch count: 1034/1073,  batch time: 0.3172s,  batch average loss: 1.0349\n",
      "Training  ------>   Batch count: 1035/1073,  batch time: 0.3437s,  batch average loss: 1.0348\n",
      "Training  ------>   Batch count: 1036/1073,  batch time: 0.2812s,  batch average loss: 1.0348\n",
      "Training  ------>   Batch count: 1037/1073,  batch time: 0.3080s,  batch average loss: 1.0347\n",
      "Training  ------>   Batch count: 1038/1073,  batch time: 0.3593s,  batch average loss: 1.0346\n",
      "Training  ------>   Batch count: 1039/1073,  batch time: 0.3750s,  batch average loss: 1.0345\n",
      "Training  ------>   Batch count: 1040/1073,  batch time: 0.3593s,  batch average loss: 1.0344\n",
      "Training  ------>   Batch count: 1041/1073,  batch time: 0.3437s,  batch average loss: 1.0344\n",
      "Training  ------>   Batch count: 1042/1073,  batch time: 0.3138s,  batch average loss: 1.0343\n",
      "Training  ------>   Batch count: 1043/1073,  batch time: 0.3080s,  batch average loss: 1.0342\n",
      "Training  ------>   Batch count: 1044/1073,  batch time: 0.3125s,  batch average loss: 1.0342\n",
      "Training  ------>   Batch count: 1045/1073,  batch time: 0.2969s,  batch average loss: 1.0341\n",
      "Training  ------>   Batch count: 1046/1073,  batch time: 0.3125s,  batch average loss: 1.0340\n",
      "Training  ------>   Batch count: 1047/1073,  batch time: 0.3437s,  batch average loss: 1.0339\n",
      "Training  ------>   Batch count: 1048/1073,  batch time: 0.3196s,  batch average loss: 1.0339\n",
      "Training  ------>   Batch count: 1049/1073,  batch time: 0.3236s,  batch average loss: 1.0338\n",
      "Training  ------>   Batch count: 1050/1073,  batch time: 0.3437s,  batch average loss: 1.0337\n",
      "Training  ------>   Batch count: 1051/1073,  batch time: 0.3437s,  batch average loss: 1.0336\n",
      "Training  ------>   Batch count: 1052/1073,  batch time: 0.2812s,  batch average loss: 1.0335\n",
      "Training  ------>   Batch count: 1053/1073,  batch time: 0.3906s,  batch average loss: 1.0334\n",
      "Training  ------>   Batch count: 1054/1073,  batch time: 0.2969s,  batch average loss: 1.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ------>   Batch count: 1055/1073,  batch time: 0.3246s,  batch average loss: 1.0333\n",
      "Training  ------>   Batch count: 1056/1073,  batch time: 0.3750s,  batch average loss: 1.0333\n",
      "Training  ------>   Batch count: 1057/1073,  batch time: 0.2969s,  batch average loss: 1.0332\n",
      "Training  ------>   Batch count: 1058/1073,  batch time: 0.3125s,  batch average loss: 1.0332\n",
      "Training  ------>   Batch count: 1059/1073,  batch time: 0.3281s,  batch average loss: 1.0331\n",
      "Training  ------>   Batch count: 1060/1073,  batch time: 0.2969s,  batch average loss: 1.0330\n",
      "Training  ------>   Batch count: 1061/1073,  batch time: 0.3080s,  batch average loss: 1.0329\n",
      "Training  ------>   Batch count: 1062/1073,  batch time: 0.3125s,  batch average loss: 1.0329\n",
      "Training  ------>   Batch count: 1063/1073,  batch time: 0.4326s,  batch average loss: 1.0328\n",
      "Training  ------>   Batch count: 1064/1073,  batch time: 0.3448s,  batch average loss: 1.0327\n",
      "Training  ------>   Batch count: 1065/1073,  batch time: 0.2911s,  batch average loss: 1.0327\n",
      "Training  ------>   Batch count: 1066/1073,  batch time: 0.3125s,  batch average loss: 1.0326\n",
      "Training  ------>   Batch count: 1067/1073,  batch time: 0.4231s,  batch average loss: 1.0325\n",
      "Training  ------>   Batch count: 1068/1073,  batch time: 0.2969s,  batch average loss: 1.0325\n",
      "Training  ------>   Batch count: 1069/1073,  batch time: 0.3125s,  batch average loss: 1.0324\n",
      "Training  ------>   Batch count: 1070/1073,  batch time: 0.2969s,  batch average loss: 1.0323\n",
      "Training  ------>   Batch count: 1071/1073,  batch time: 0.3125s,  batch average loss: 1.0322\n",
      "Training  ------>   Batch count: 1072/1073,  batch time: 0.3423s,  batch average loss: 1.0322\n",
      "Training  ------>   Batch count: 1073/1073,  batch time: 0.3675s,  batch average loss: 1.0321\n",
      "Training time: 362.8538s, loss: 1.0321, accuracy: 45.9886%\n",
      "-------------------------------------------------- Validating epoch 0 --------------------------------------------------\n",
      "Testing  ------>   Batch count: 1/20,  batch time: 0.0839s,  batch average loss: 0.9133\n",
      "Testing  ------>   Batch count: 2/20,  batch time: 0.0819s,  batch average loss: 0.9113\n",
      "Testing  ------>   Batch count: 3/20,  batch time: 0.0747s,  batch average loss: 0.9092\n",
      "Testing  ------>   Batch count: 4/20,  batch time: 0.0744s,  batch average loss: 0.9081\n",
      "Testing  ------>   Batch count: 5/20,  batch time: 0.1069s,  batch average loss: 0.9071\n",
      "Testing  ------>   Batch count: 6/20,  batch time: 0.0839s,  batch average loss: 0.9030\n",
      "Testing  ------>   Batch count: 7/20,  batch time: 0.0850s,  batch average loss: 0.9083\n",
      "Testing  ------>   Batch count: 8/20,  batch time: 0.0849s,  batch average loss: 0.9126\n",
      "Testing  ------>   Batch count: 9/20,  batch time: 0.0860s,  batch average loss: 0.9100\n",
      "Testing  ------>   Batch count: 10/20,  batch time: 0.0940s,  batch average loss: 0.9085\n",
      "Testing  ------>   Batch count: 11/20,  batch time: 0.1209s,  batch average loss: 0.9096\n",
      "Testing  ------>   Batch count: 12/20,  batch time: 0.0800s,  batch average loss: 0.9094\n",
      "Testing  ------>   Batch count: 13/20,  batch time: 0.1140s,  batch average loss: 0.9075\n",
      "Testing  ------>   Batch count: 14/20,  batch time: 0.0822s,  batch average loss: 0.9066\n",
      "Testing  ------>   Batch count: 15/20,  batch time: 0.0872s,  batch average loss: 0.9050\n",
      "Testing  ------>   Batch count: 16/20,  batch time: 0.0787s,  batch average loss: 0.9061\n",
      "Testing  ------>   Batch count: 17/20,  batch time: 0.0870s,  batch average loss: 0.9070\n",
      "Testing  ------>   Batch count: 18/20,  batch time: 0.0859s,  batch average loss: 0.9052\n",
      "Testing  ------>   Batch count: 19/20,  batch time: 0.0880s,  batch average loss: 0.9064\n",
      "Testing  ------>   Batch count: 20/20,  batch time: 0.0210s,  batch average loss: 0.9066\n",
      "Validating time: 1.9186s, loss: 0.9066, accuracy: 58.3316%\n",
      "\n",
      "-------------------------------------------------- Training epoch 1 --------------------------------------------------\n",
      "Training  ------>   Batch count: 1/1073,  batch time: 0.3465s,  batch average loss: 0.9778\n",
      "Training  ------>   Batch count: 2/1073,  batch time: 0.2928s,  batch average loss: 0.9621\n",
      "Training  ------>   Batch count: 3/1073,  batch time: 0.2826s,  batch average loss: 0.9569\n",
      "Training  ------>   Batch count: 4/1073,  batch time: 0.3525s,  batch average loss: 0.9541\n",
      "Training  ------>   Batch count: 5/1073,  batch time: 0.3070s,  batch average loss: 0.9570\n",
      "Training  ------>   Batch count: 6/1073,  batch time: 0.3650s,  batch average loss: 0.9580\n",
      "Training  ------>   Batch count: 7/1073,  batch time: 0.3218s,  batch average loss: 0.9570\n",
      "Training  ------>   Batch count: 8/1073,  batch time: 0.3008s,  batch average loss: 0.9592\n",
      "Training  ------>   Batch count: 9/1073,  batch time: 0.3228s,  batch average loss: 0.9572\n",
      "Training  ------>   Batch count: 10/1073,  batch time: 0.3159s,  batch average loss: 0.9590\n",
      "Training  ------>   Batch count: 11/1073,  batch time: 0.3148s,  batch average loss: 0.9569\n",
      "Training  ------>   Batch count: 12/1073,  batch time: 0.4043s,  batch average loss: 0.9583\n",
      "Training  ------>   Batch count: 13/1073,  batch time: 0.2955s,  batch average loss: 0.9586\n",
      "Training  ------>   Batch count: 14/1073,  batch time: 0.4167s,  batch average loss: 0.9578\n",
      "Training  ------>   Batch count: 15/1073,  batch time: 0.3061s,  batch average loss: 0.9583\n",
      "Training  ------>   Batch count: 16/1073,  batch time: 0.3190s,  batch average loss: 0.9591\n",
      "Training  ------>   Batch count: 17/1073,  batch time: 0.3235s,  batch average loss: 0.9588\n",
      "Training  ------>   Batch count: 18/1073,  batch time: 0.3578s,  batch average loss: 0.9582\n",
      "Training  ------>   Batch count: 19/1073,  batch time: 0.3628s,  batch average loss: 0.9598\n",
      "Training  ------>   Batch count: 20/1073,  batch time: 0.3074s,  batch average loss: 0.9600\n",
      "Training  ------>   Batch count: 21/1073,  batch time: 0.3301s,  batch average loss: 0.9597\n",
      "Training  ------>   Batch count: 22/1073,  batch time: 0.3679s,  batch average loss: 0.9605\n",
      "Training  ------>   Batch count: 23/1073,  batch time: 0.3452s,  batch average loss: 0.9606\n",
      "Training  ------>   Batch count: 24/1073,  batch time: 0.3208s,  batch average loss: 0.9604\n",
      "Training  ------>   Batch count: 25/1073,  batch time: 0.3798s,  batch average loss: 0.9608\n",
      "Training  ------>   Batch count: 26/1073,  batch time: 0.3514s,  batch average loss: 0.9617\n",
      "Training  ------>   Batch count: 27/1073,  batch time: 0.3022s,  batch average loss: 0.9604\n",
      "Training  ------>   Batch count: 28/1073,  batch time: 0.2988s,  batch average loss: 0.9598\n",
      "Training  ------>   Batch count: 29/1073,  batch time: 0.3391s,  batch average loss: 0.9576\n",
      "Training  ------>   Batch count: 30/1073,  batch time: 0.3048s,  batch average loss: 0.9571\n",
      "Training  ------>   Batch count: 31/1073,  batch time: 0.3438s,  batch average loss: 0.9579\n",
      "Training  ------>   Batch count: 32/1073,  batch time: 0.3658s,  batch average loss: 0.9564\n",
      "Training  ------>   Batch count: 33/1073,  batch time: 0.2857s,  batch average loss: 0.9559\n",
      "Training  ------>   Batch count: 34/1073,  batch time: 0.3127s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 35/1073,  batch time: 0.3258s,  batch average loss: 0.9539\n",
      "Training  ------>   Batch count: 36/1073,  batch time: 0.2948s,  batch average loss: 0.9544\n",
      "Training  ------>   Batch count: 37/1073,  batch time: 0.3108s,  batch average loss: 0.9545\n",
      "Training  ------>   Batch count: 38/1073,  batch time: 0.3698s,  batch average loss: 0.9550\n",
      "Training  ------>   Batch count: 39/1073,  batch time: 0.3060s,  batch average loss: 0.9552\n",
      "Training  ------>   Batch count: 40/1073,  batch time: 0.3001s,  batch average loss: 0.9544\n",
      "Training  ------>   Batch count: 41/1073,  batch time: 0.3498s,  batch average loss: 0.9539\n",
      "Training  ------>   Batch count: 42/1073,  batch time: 0.3118s,  batch average loss: 0.9545\n",
      "Training  ------>   Batch count: 43/1073,  batch time: 0.3298s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 44/1073,  batch time: 0.3007s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 45/1073,  batch time: 0.3004s,  batch average loss: 0.9552\n",
      "Training  ------>   Batch count: 46/1073,  batch time: 0.2989s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 47/1073,  batch time: 0.2858s,  batch average loss: 0.9543\n",
      "Training  ------>   Batch count: 48/1073,  batch time: 0.3058s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 49/1073,  batch time: 0.3188s,  batch average loss: 0.9545\n",
      "Training  ------>   Batch count: 50/1073,  batch time: 0.3258s,  batch average loss: 0.9541\n",
      "Training  ------>   Batch count: 51/1073,  batch time: 0.3078s,  batch average loss: 0.9546\n",
      "Training  ------>   Batch count: 52/1073,  batch time: 0.2831s,  batch average loss: 0.9549\n",
      "Training  ------>   Batch count: 53/1073,  batch time: 0.2895s,  batch average loss: 0.9552\n",
      "Training  ------>   Batch count: 54/1073,  batch time: 0.2998s,  batch average loss: 0.9553\n",
      "Training  ------>   Batch count: 55/1073,  batch time: 0.3238s,  batch average loss: 0.9554\n",
      "Training  ------>   Batch count: 56/1073,  batch time: 0.3148s,  batch average loss: 0.9546\n",
      "Training  ------>   Batch count: 57/1073,  batch time: 0.3216s,  batch average loss: 0.9543\n",
      "Training  ------>   Batch count: 58/1073,  batch time: 0.3285s,  batch average loss: 0.9537\n",
      "Training  ------>   Batch count: 59/1073,  batch time: 0.3228s,  batch average loss: 0.9533\n",
      "Training  ------>   Batch count: 60/1073,  batch time: 0.3278s,  batch average loss: 0.9535\n",
      "Training  ------>   Batch count: 61/1073,  batch time: 0.3478s,  batch average loss: 0.9530\n",
      "Training  ------>   Batch count: 62/1073,  batch time: 0.3068s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 63/1073,  batch time: 0.3078s,  batch average loss: 0.9526\n",
      "Training  ------>   Batch count: 64/1073,  batch time: 0.2853s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 65/1073,  batch time: 0.3572s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 66/1073,  batch time: 0.3081s,  batch average loss: 0.9517\n",
      "Training  ------>   Batch count: 67/1073,  batch time: 0.3468s,  batch average loss: 0.9516\n",
      "Training  ------>   Batch count: 68/1073,  batch time: 0.3038s,  batch average loss: 0.9515\n",
      "Training  ------>   Batch count: 69/1073,  batch time: 0.3294s,  batch average loss: 0.9518\n",
      "Training  ------>   Batch count: 70/1073,  batch time: 0.3062s,  batch average loss: 0.9517\n",
      "Training  ------>   Batch count: 71/1073,  batch time: 0.2948s,  batch average loss: 0.9517\n",
      "Training  ------>   Batch count: 72/1073,  batch time: 0.2921s,  batch average loss: 0.9515\n",
      "Training  ------>   Batch count: 73/1073,  batch time: 0.3548s,  batch average loss: 0.9516\n",
      "Training  ------>   Batch count: 74/1073,  batch time: 0.3138s,  batch average loss: 0.9515\n",
      "Training  ------>   Batch count: 75/1073,  batch time: 0.2928s,  batch average loss: 0.9515\n",
      "Training  ------>   Batch count: 76/1073,  batch time: 0.2776s,  batch average loss: 0.9518\n",
      "Training  ------>   Batch count: 77/1073,  batch time: 0.3020s,  batch average loss: 0.9515\n",
      "Training  ------>   Batch count: 78/1073,  batch time: 0.4227s,  batch average loss: 0.9514\n",
      "Training  ------>   Batch count: 79/1073,  batch time: 0.3678s,  batch average loss: 0.9520\n",
      "Training  ------>   Batch count: 80/1073,  batch time: 0.3258s,  batch average loss: 0.9524\n",
      "Training  ------>   Batch count: 81/1073,  batch time: 0.2860s,  batch average loss: 0.9526\n",
      "Training  ------>   Batch count: 82/1073,  batch time: 0.3044s,  batch average loss: 0.9526\n",
      "Training  ------>   Batch count: 83/1073,  batch time: 0.3593s,  batch average loss: 0.9530\n",
      "Training  ------>   Batch count: 84/1073,  batch time: 0.3311s,  batch average loss: 0.9527\n",
      "Training  ------>   Batch count: 85/1073,  batch time: 0.2818s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 86/1073,  batch time: 0.2879s,  batch average loss: 0.9524\n",
      "Training  ------>   Batch count: 87/1073,  batch time: 0.3738s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 88/1073,  batch time: 0.4231s,  batch average loss: 0.9523\n",
      "Training  ------>   Batch count: 89/1073,  batch time: 0.3129s,  batch average loss: 0.9523\n",
      "Training  ------>   Batch count: 90/1073,  batch time: 0.2928s,  batch average loss: 0.9522\n",
      "Training  ------>   Batch count: 91/1073,  batch time: 0.3408s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 92/1073,  batch time: 0.3490s,  batch average loss: 0.9521\n",
      "Training  ------>   Batch count: 93/1073,  batch time: 0.2818s,  batch average loss: 0.9520\n",
      "Training  ------>   Batch count: 94/1073,  batch time: 0.3109s,  batch average loss: 0.9525\n",
      "Training  ------>   Batch count: 95/1073,  batch time: 0.3318s,  batch average loss: 0.9524\n",
      "Training  ------>   Batch count: 96/1073,  batch time: 0.3268s,  batch average loss: 0.9522\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-a450e608b413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Training epoch %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mepoch_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch_accuracy\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-3ac5ff0f7258>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, criterion, max_gradient_norm)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpremises\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpremises_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhypothesis_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FduNLP-beginner-task\\task3\\model\\esim.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, premises, premises_lengths, hypotheses, hypotheses_lengths)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         encoded_premises = self._encoding(embedded_premises,\n\u001b[1;32m--> 127\u001b[1;33m                                           premises_lengths)\n\u001b[0m\u001b[0;32m    128\u001b[0m         encoded_hypotheses = self._encoding(embedded_hypotheses,\n\u001b[0;32m    129\u001b[0m                                             hypotheses_lengths)\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\FduNLP-beginner-task\\task3\\model\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sequences_batch, sequences_lengths)\u001b[0m\n\u001b[0;32m    117\u001b[0m                                                          batch_first=True)\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[1;32m--> 529\u001b[1;33m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练过程中的参数\n",
    "best_score = 0.0\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "patience_cnt = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"-\"*50, \"Training epoch %d\" % (epoch), \"-\"*50)\n",
    "    epoch_time, epoch_loss, epoch_accuracy = train(model, train_loader, optimizer, criterion,max_grad_norm)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(\"Training time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\".format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "    \n",
    "    print(\"-\"*50, \"Validating epoch %d\"%(epoch), \"-\"*50)\n",
    "    epoch_time_dev, epoch_loss_dev, epoch_accuracy_dev = validate(model, dev_loader, criterion)\n",
    "    valid_losses.append(epoch_loss_dev)\n",
    "    print(\"Validating time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\\n\".format(epoch_time_dev, epoch_loss_dev, (epoch_accuracy_dev*100)))\n",
    "    \n",
    "    #更新学习率\n",
    "    scheduler.step(epoch_accuracy)\n",
    "    \n",
    "    #early stoping\n",
    "    if epoch_accuracy_dev < best_score:\n",
    "        patience_cnt += 1\n",
    "    else:\n",
    "        best_score = epoch_accuracy_dev\n",
    "        patience_cnt = 0\n",
    "    if patience_cnt >= patience:\n",
    "        print(\"-\"*50, \"Early stopping\", \"-\"*50)\n",
    "        break\n",
    "        \n",
    "    #每个epoch都保存模型\n",
    "    torch.save({\"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"best_score\": best_score,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses},\n",
    "               model_train_dir + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Dropout): # 这个继承自带了self.training\n",
    "    \"\"\"\n",
    "    Dropout layer for the inputs of RNNs.\n",
    "\n",
    "    Apply the same dropout mask to all the elements of the same sequence in\n",
    "    a batch of sequences of size (batch, sequences_length, embedding_dim).\n",
    "    \"\"\"\n",
    "    def forward(self, sequences_batch):\n",
    "        \"\"\"\n",
    "        Apply dropout to the input batch of sequences.\n",
    "\n",
    "        Args:\n",
    "            sequences_batch: A batch of sequences of vectors that will serve\n",
    "                as input to an RNN.\n",
    "                Tensor of size (batch, sequences_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            A new tensor on which dropout has been applied.\n",
    "        \"\"\"\n",
    "        ones = sequences_batch.data.new_ones(sequences_batch.shape[0], 1, sequences_batch.shape[-1])\n",
    "        print(\"ones\", ones)\n",
    "        dropout_mask = nn.functional.dropout(ones, self.p, self.training, inplace=False) # 最好用nn.Dropout\n",
    "        print(\"drop\", dropout_mask)\n",
    "        return dropout_mask * sequences_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNDropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones tensor([[[1., 1., 1., 1., 1.]]])\n",
      "drop tensor([[[0., 0., 0., 2., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.0000, -0.0000,  2.3526, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.6429, -0.0000]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,3],[4,2],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[1, 2],\n",
       "        [4, 3],\n",
       "        [5, 6]]),\n",
       "indices=tensor([[0, 1],\n",
       "        [1, 0],\n",
       "        [2, 2]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
