{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "dir_all_data='data/train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_all.shape:  (156060, 4)\n",
      "data_all.keys:  Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n"
     ]
    }
   ],
   "source": [
    "#读取数据 \n",
    "data_all = pd.read_csv(dir_all_data, sep='\\t')\n",
    "print(\"data_all.shape: \", data_all.shape)    #(156060, 4)\n",
    "print(\"data_all.keys: \", data_all.keys())   #['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']\n",
    "print(data_all.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060,)\n",
      "(156060,)\n"
     ]
    }
   ],
   "source": [
    "#取出要处理的列\n",
    "x_all = data_all['Phrase']\n",
    "y_all = data_all['Sentiment']\n",
    "print(x_all.shape)   #(156060,)\n",
    "print(y_all.shape)   #(156060,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636,) (31212,) (31212,)\n"
     ]
    }
   ],
   "source": [
    "#划分验证集、测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)\n",
    "print(x_train.shape, x_val.shape, x_test.shape)   #(93636,) (31212,) (31212,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 15202) (31212, 15202)\n",
      "5808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取文本计数特征 -- 每个单词的数量\n",
    "#对文本的单词进行计数，包括文本的预处理, 分词以及过滤停用词\n",
    "count_vect = CountVectorizer()  \n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_test_counts = count_vect.transform(x_test)\n",
    "print(x_train_counts.shape, x_test_counts.shape)  #(93636, 15188) (31212, 15188)  矩阵(句子-词汇）的维度，词表大小15188\n",
    "#在词汇表中一个单词的索引值对应的是该单词在整个训练的文集中出现的频率。\n",
    "print(count_vect.vocabulary_.get('good'))    #5808     count_vect.vocabulary_是一个词典：word-id\n",
    "# x_train_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 15202) (31212, 15202)\n"
     ]
    }
   ],
   "source": [
    "#提取TF-IDF特征 -- word级别的TF-IDF\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_word = tfidf_transformer.transform(x_train)\n",
    "x_test_tfidf_word = tfidf_transformer.transform(x_test)\n",
    "print(x_train_tfidf_word.shape, x_test_tfidf_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 50000) (31212, 50000)\n"
     ]
    }
   ],
   "source": [
    "#提取TF-IDF特征 - ngram级别的TF-IDF\n",
    "#将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频tf。\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', ngram_range=(2,3), max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_ngram = tfidf_transformer.transform(x_train)\n",
    "x_test_tfidf_ngram = tfidf_transformer.transform(x_test)\n",
    "print(x_train_tfidf_ngram.shape, x_test_tfidf_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93636, 80404)\n"
     ]
    }
   ],
   "source": [
    "#合并特征（特征组合与特征选择）\n",
    "train_features = x_train_counts\n",
    "test_features = x_test_counts\n",
    "train_features = hstack([x_train_counts, x_train_tfidf_word, x_train_tfidf_ngram])\n",
    "test_features = hstack([x_test_counts, x_test_tfidf_word, x_test_tfidf_ngram])\n",
    "print(train_features.shape)   #特征的最终维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练分类器\n",
    "\n",
    "#逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                         solver='sag', #优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                         multi_class='multinomial' #分类方式：multinomial、ovr\n",
    ")\n",
    "\n",
    "#朴素贝叶斯\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#clf = MultinomialNB().fit(train_features, y_train)\n",
    "\n",
    "\n",
    "#SGDClassifier是一系列采用了梯度下降来求解参数的算法的集合，默认是SVM\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# clf = SGDClassifier(alpha=0.001,\n",
    "#                     loss='log',    #hinge代表SVM，log是逻辑回归\n",
    "#                     early_stopping=True,\n",
    "#                     eta0=0.001,\n",
    "#                     learning_rate='adaptive', #constant、optimal、invscaling、adaptive\n",
    "#                     max_iter=100 \n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#打乱数据，训练\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_features, y_train = shuffle(train_features, y_train)\n",
    "clf.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6570549788542868\n"
     ]
    }
   ],
   "source": [
    "#测试过程\n",
    "predict = clf.predict(test_features)\n",
    "#测试集的评估\n",
    "print(np.mean(predict == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
